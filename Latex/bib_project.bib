% Encoding: UTF-8

@Article{FWBQ,
  author       = {François-Xavier Briol and Chris J. Oates and Mark Girolami and Michael A. Osborne},
  title        = {Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees},
  abstract     = {There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.},
  date         = {2015-06-08},
  eprint       = {https://arxiv.org/pdf/1506.02681.pdf},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1506.02681v3:PDF},
  journaltitle = {Advances in Neural Information Processing Systems 28, 1162--1170, 2015},
  keywords     = {stat.ML},
}

@Unpublished{duvenaud,
  author = {David Duvenaud},
  title  = {Bayesian Quadrature: Model-based Approximate Integration},
  url    = {https://www.cs.toronto.edu/~duvenaud/talks/intro_bq.pdf},
  Howpublished = {University Lecture},
  Institution = {University of Cambridge},
}

@Unpublished{fwalgo,
  author = {Michael Patriksson},
  title  = {The Frank–Wolfe algorithm},
  url    = {http://www.math.chalmers.se/Math/Grundutb/CTH/tma946/0203/fw_eng.pdf},
  Howpublished = {University Lecture},
  Institution = {University of Göteborgs},
}

@Article{huszar,
  author      = {Ferenc Huszár and David Duvenaud},
  title       = {Optimally-Weighted Herding is Bayesian Quadrature},
  abstract    = {Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate.},
  date        = {2012-04-07},
  eprint      = {https://arxiv.org/pdf/1204.1664.pdf},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1204.1664v3:PDF},
  keywords    = {stat.ML, math.NA, G.1.4},
}

@article{Beck,
author = {Beck, Amir and Teboulle, Marc},
year = {2004},
month = {01},
pages = {235-247},
title = {A conditional gradient method with linear rate of convergence for solving convex linear systems},
volume = {59},
journal = {Mathematical Methods of Operations Research},
doi = {10.1007/s001860300327}
}

@Article{Bach,
  author       = {Francis Bach and Simon Lacoste-Julien and Guillaume Obozinski},
  title        = {On the Equivalence between Herding and Conditional Gradient Algorithms},
  abstract     = {We show that the herding procedure of Welling (2009) takes exactly the form of a standard convex optimization algorithm--namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. The experiments indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm tends to approach more often the maximum entropy distribution, shedding more light on the learning bias behind herding.},
  date         = {2012-03-20},
  eprint       = {http://arxiv.org/abs/1203.4523v2},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1203.4523v2:PDF},
  journaltitle = {ICML 2012 International Conference on Machine Learning, Edimburgh : Royaume-Uni (2012)},
  keywords     = {cs.LG, math.OC, stat.ML},
}

@Article{Chen,
  author      = {Yutian Chen and Max Welling and Alex Smola},
  title       = {Super-Samples from Kernel Herding},
  abstract    = {We extend the herding algorithm to continuous spaces by using the kernel trick. The resulting "kernel herding" algorithm is an infinite memory deterministic process that learns to approximate a PDF with a collection of samples. We show that kernel herding decreases the error of expectations of functions in the Hilbert space at a rate O(1/T) which is much faster than the usual O(1/pT) for iid random samples. We illustrate kernel herding by approximating Bayesian predictive distributions.},
  date        = {2012-03-15},
  eprint      = {http://arxiv.org/abs/1203.3472v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1203.3472v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Comment{jabref-meta: databaseType:bibtex;}
