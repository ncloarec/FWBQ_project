\section*{Notations}
We start with some notations we will use along this report.
\begin{itemize}[font= \color{blue} \large, label= $\bullet$]
 \item For any function, $g(\cdot)$ denotes the function $g: x \mapsto g(x)$.
 \item In integrals, $\d x$ denote $\d \lambda(x)$, \ie with respect to the Lebesgue measure.
\end{itemize}

\section*{Introduction}

The goal of the article \cite{FWBQ} is to compute efficiently the integrals of the form
$ \displaystyle \int _ { \X } f ( x ) p ( x ) \mathrm { d } x$
where $\X \subseteq \R ^ { d }$ is a measurable space,
$d \geq 1$ integer representing the dimension of the problem, $p$ a probability
density with respect to the Lebesgue measure on $\X$ and $f : \X \rightarrow \R$
 is a \textit{test}-function.

 We will use the common approximation
 \begin{boxproblem}
   \begin{equation}
  \int _ { \X } f ( x ) p ( x ) \d x \approx \sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right)
   \end{equation}
 \end{boxproblem}

 but of course the real challenge lies in the choice of sequences $\acc{x_i}$ and
 $\acc{w_i}$ :
  \begin{itemize}[font= \color{blue} \large, label= $\bullet$]
    \item \textbf{Monte Carlo}: $w_i = \frac{1}{n}$ and $x_i$ realization of multivariate
    random variable $X_i \stackrel{iid}{\sim} X$ where $X$ has $p(\cdot)$ as probability
    distribution.
    \item \textbf{Kernel herding}:
    \item \textbf{Quasi-Monte Carlo}:
  \end{itemize}

  In the \textbf{Frank-Wolfe Bayesian Quadrature}, we have
  \begin{itemize}[font= \color{blue} \large, label= \ding{43}]
\item $\acc{w_i}$ which appear naturally in the Bayesian Quadrature by taking the expectation of a posterior distribution  (described in section \ref{sec:BQ}),
\item $\acc{x_i}$ selected by the Frank-Wolfe algorithm in order to minimize a posterior variance (described in section \ref{sec:FW}).
  \end{itemize}

  The main interest of the method developed in \cite{FWBQ} is the super fast
  \textit{exponential} convergence to the true value of the integral compared to the other methods mentioned above.\\

  Through this report, we will detail every results from \cite{FWBQ} with the goal
  to clarify and explain details that could have been omitted intentionally or not and which, in our view, make the Briol's and al. approach more natural, intuitive and easier
  to understand.
  % use Bach presentation for convergence rates benchmark

\section{Background}
% write properly the RKHS
% write $\mu_p$ with an integral
% MMD formula
%WHY RKHS, WHY so USEFUL ?
Let $\X \subseteq \R ^ { d }$ be a measurable space, $\mu$ a measure on $\X$ such
that $p = \frac{\d \mu}{ \d \lambda}$ where $\lambda$ denotes the Lebesgue measure on $\X$,
  $\H \subset L^2(\X, \R; \mu)$ be an RKHS with a
 reproducing kernel $k: \X \times \X \rightarrow \R$, $\Phi$ its canonical feature
 map associated. We denote respectively by $\bigps{\cdot}{\cdot}$ and $\norme{\cdot}$
 the bigps product and norm induced on $\H$.

 Recall that the following relations hold:
 \begin{boxexercise}
   \begin{align}
  \forall x \in \X , \quad &k ( \cdot , x ) \in \H\\
  \forall x \in \X , \forall f \in \H , \quad &\langle f , k ( \cdot , x ) \rangle _ { \H } = f ( x )\\
  \forall (x,y) \in \X^2 \quad &k(x,y) = \bigps{\Phi(x)}{\Phi(y)}
  \end{align}
\end{boxexercise}


Let's denote as \cite{FWBQ}:
\begin{boxdefinition}
  \begin{align*}
    p[f] &:= \int _ { \X } f ( x ) \mathrm { d } \mu(x) = \int _ { \X } f ( x ) p ( x ) \mathrm { d } x\\
    \p[f]&:=\sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right).
  \end{align*}
\end{boxdefinition}


We will use the \textit{maximum mean discrepancy} (MMD) as our main metric to measure
 the accuracy of the approximation $p[f] \approx \p[f]$ in the worst case scenario and which is defined as
 \begin{boxdefinition}
    $$\MMD : = \sup _ { f \in \H : \norme{f}=1 } | p [ f ] - \hat { p } [ f ] |.$$
 \end{boxdefinition}

 Let's show (formula 3. in \cite{FWBQ}) that MMD can be rewrite as
 \begin{boxtheorem}
   \begin{align}
     \MMD = \left\| \mu _ { p } - \mu _ { \hat { p } } \right\| _ { \mathcal { H } }
   \end{align}
   where $\mu_p(\cdot) = p[\Phi(\cdot)]$ and $\mu_{\p}(\cdot) = \p[\Phi(\cdot)]$.
 \end{boxtheorem}

 \begin{boxitemize}
   \begin{itemize}[leftmargin=*, font= \color{blue} \large, label= $\bullet$]
     \item \textbf{For all $f$ in $\H$, we have $p[f] = \bigps{f}{\mu_p}$.} By using
     the dirac delta function, the continuity of the inner product and viewing integral
     as a limit of a sum, we get
     \begin{align*}
       p[f] &= \int_{\X} f(x) \d \mu(x)\\
       &= \int_{\X} \delta_x[f]  \d \mu(x)\\
       &= \int_{\X} \bigps{f}{\Phi(x)}  \d \mu(x)\\%TODO Not obvious
       &=  \bigps{f}{\int_{\X} \Phi(x) \d \mu(x)} \\
       &= \bigps{f}{\mu_p}
     \end{align*}
     \item \textbf{For all $f$ in $\H$, we have $\p[f] = \bigps{f}{\mu_{\p}}$.}
     \begin{align*}
       \p[f] &= \sum_{i=1}^n w_i f(x_i)\\
       &= \sum_{i=1}^n w_i \delta_{x_i}[f]\\
       &= \sum_{i=1}^n w_i \bigps{f}{\Phi(x_i)}\\%TODO Not obvious
       &=  \bigps{f}{\sum_{i=1}^n w_i \Phi(x_i)} \\
       &= \bigps{f}{\mu_{\p}}
     \end{align*}
\item By using previous results and the Cauchy-schwartz inequality, we get :
\begin{align*}
\MMD &= \sup _ { f \in \H : \norme{f} = 1 } \abs{
\bigps{f}{\mu_{p}-\mu_{\p}} }\\
&\leq \sup _ { f \in \H : \norme{f} = 1 } \norme{f} \norme{\mu_{p}-\mu_{\p}}\\
&= \norme{\mu_{p}-\mu_{\p}}
\end{align*}
with equality if and only if $f$ and  $\mu_{p}-\mu_{\p}$ are linearly dependent.
We deduce the desired result by taking $ \displaystyle f = \frac{1}{\norme{\mu_{p}-\mu_{\p}}}\prt{\mu_{p}-\mu_{\p}}$.

   \end{itemize}
 \end{boxitemize}


% on maximise avec cauchy schwartz + égalité

\section{Bayesian Quadrature}
\label{sec:BQ}
Let's place a functional prior on the integrand $f$ and denote by $(\Omega, \mathcal{F}, \P)$ its probability space associated. We will assume that $f$ to be
 a \textit{centered} \textbf{gaussian process} with the kernel $k$ as its covariance
 function, \ie
 \begin{boxexample}
   \begin{align*}
     \forall x \in \H, \quad  &\Expec{f(x)} = 0\\
     \forall x, y \in \H, \quad  &\Cov{f(x)}{f(y)} = k(x,y)
   \end{align*}
 \end{boxexample}
A useful property is that $p[f]$ is a gaussian variable and then completely defined by its second-order statistics:
\begin{boxtheorem}
  \begin{align}
    \Expec{p[f]} &= 0\\
    \Var{p[f]} &= \int_{\X^2} k(x,y) \d \mu(x) \d \mu(y)
  \end{align}
\end{boxtheorem}

\begin{boxitemize}
  By switching integrals using Fubini's theorem, we get
    \begin{align*}
      \Expec{p[f]} &= \int_{\Omega} p[f](w) \d \P(w)\\
      &= \int_{\Omega} \int_{\X} f(x,w) \d \mu(x)  \d \P(w)\\
      &= \int_{\X} \underbrace{\int_{\Omega}  f(x,w) \d \P(w)}_{\Expec{f(x)}=0} \d \mu(x)  \\
      &=0\\
      \Var{p[f]} &= \Expec{p[f]^2} = \int_{\Omega} p[f](w)^2 \d \P(w)\\
      &=\int_{\Omega} \prt{\int_{\X} f(x,w) \d \mu(x)}^2 \d \P(w)\\
      &=\int_{\Omega} \prt{\int_{\X^2} f(x,w)f(y,w) \d \mu(x) \d \mu(y)} \d \P(w)\\
      &= \int_{\X^2} \underbrace{\int_{\Omega}  f(x,w)f(y,w) \d \P(w)}_{=\Cov{f(x)}{f(y)} = k(x,y)} \d \mu(x) \d \mu(y) \\
      &= \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y)
    \end{align*}
\end{boxitemize}

Assume that samples $\acc{x_i}$ and $\acc{f_i} := \acc{f(x_i)}$ are given for $i=1$ to $n$. How to update the weights $\acc{w_i}_{i=1}^n $?

\begin{figure}[H]
\centering
\includegraphics[scale=.20]{bq.png}
\caption{Bayesian quadrature approach. \textit{Source:} \cite{duvenaud}}
\label{fig:bq}
\end{figure}

First of all, let's determine the conditional distribution $\displaystyle
 p[f]| \; \mathbf{f}$ where
$\mathbf{f} = \transpose{\prt{f_1, \ldots, f_n}}$.
Since both $p[f]$ and $\mathbf{f}$ are gaussian, we can use the conditional gaussian rule:
\begin{boxtheorem}
  By denoting, $y_1 := p[f]$, $y_2 = \mathbf{f}$, $y = \begin{pmatrix}
    y_1\\y_2
\end{pmatrix}$, $\displaystyle \Sigma = \begin{pmatrix}
  \Sigma_{1,1}, \Sigma_{1,2}\\
  \Sigma_{2,1}, \Sigma_{2,2}
\end{pmatrix}$ its covariance matrix by blocks, we have :
$$p[f]| \; \mathbf{f} \sim \mathcal { N } (  \mu  , \Stilde )$$
where

  %
  %

  \begin{equation*}
  \begin{cases}
    \mu \;&= \;  \Sigma _ { 12 } \; \Sigma _ { 22 } ^ { - 1 } \; \mathbf{f} \\
     \Stilde \; &= \;  \Sigma _ { 11 } \;- \Sigma _ { 12 } \;\Sigma _ { 22 } ^ { - 1 } \; \Sigma _ { 21 }.
  \end{cases}
\end{equation*}
\end{boxtheorem}

% Conditional rule demonstrated
% Demonstration formula 4
% Demonstration formula 5

\section{Frank-Wolfe algorithm}
\label{sec:FW}
% intuition
% graph to show that it is closed to a gradient descent
% drawing line search Aspremont

\section*{Resources}
\label{sec:bibli}

\nocite{*}
\printbibliography[heading=none]
