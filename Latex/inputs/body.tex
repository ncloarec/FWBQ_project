\section*{Notations}
We start with some notations we will use along this report.
\begin{itemize}[font= \color{blue} \large, label= $\bullet$]
 \item For any function, $g(\cdot)$ denotes the function $g: x \mapsto g(x)$.
\end{itemize}

\section*{Introduction}

The goal of the article \citet{FWBQ} is to compute efficiently the integrals of the form
$ \displaystyle \int _ { \X } f ( x ) p ( x ) \mathrm { d } x$
where $\X \subseteq \R ^ { d }$ is a measurable space,
$d \geq 1$ integer representing the dimension of the problem, $p$ a probability
density with respect to the Lebesgue measure on $\X$ and $f : \X \rightarrow \R$
 is a \textit{test}-function.

 We will use the common approximation
 \begin{boxproblem}
   \begin{equation}
  \int _ { \X } f ( x ) p ( x ) \d x \approx \sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right)
   \end{equation}
 \end{boxproblem}

 but of course the real challenge lies in the choice of sequences $\acc{x_i}$ and
 $\acc{w_i}$ :
  \begin{itemize}[font= \color{blue} \large, label= $\bullet$]
    \item \textbf{Monte Carlo}: $w_i = \frac{1}{n}$ and $x_i$ realization of multivariate
    random variable $X_i \stackrel{iid}{\sim} X$ where $X$ has $p(\cdot)$ as probability
    distribution.
    \item \textbf{Kernel herding}:
    \item \textbf{Quasi-Monte Carlo}:
  \end{itemize}

  In the \textbf{Frank-Wolfe Bayesian Quadrature}, we have
  \begin{itemize}[font= \color{blue} \large, label= \ding{43}]
\item $\acc{w_i}$ which appear naturally in the Bayesian Quadrature by taking the expectation of a posterior distribution  (described in section \ref{sec:BQ}),
\item $\acc{x_i}$ selected by the Frank-Wolfe algorithm in order to minimize a posterior variance (described in section \ref{sec:FW}).
  \end{itemize}

  The main interest of the method developed in \citet{FWBQ} is the super fast
  \textit{exponential} convergence to the true value of the integral compared to the other methods mentioned above.\\

  Through this report, we will detail every results from \citet{FWBQ} with the goal
  to clarify and explain details that could have been omitted intentionally or not and which, in our view, make the Briol's and al. approach more natural, intuitive and easier
  to understand.
  % use Bach presentation for convergence rates benchmark

\section{Background}
% write properly the RKHS
% write $\mu_p$ with an integral
% MMD formula
%WHY RKHS, WHY so USEFUL ?
Let $\X \subseteq \R ^ { d }$ be a measurable space, $\mu$ a measure on $\X$ such
that $p = \frac{\d \mu}{ \d \lambda}$ where $\lambda$ denotes the Lebesgue measure on $\X$,
  $\H \subset L^2(\X; \mu)$ be an RKHS with a
 reproducing kernel $k: \X \times \X \rightarrow \R$, $\Phi$ its canonical feature
 map associated. We denote respectively by $\inner{\cdot}{\cdot}_{H}$ and $\norme{\cdot}_{\H}$
 the inner product and norm induced on $\H$.

 Recall that the following relations hold:
 \begin{boxexercise}
   \begin{align}
  \forall x \in \X , \quad &k ( \cdot , x ) \in \H\\
  \forall x \in \X , \forall f \in \H , \quad &\langle f , k ( \cdot , x ) \rangle _ { \H } = f ( x )\\
  \forall (x,y) \in \X^2 \quad &k(x,y) = \inner{\Phi(x)}{\Phi(y)}_{\H}
  \end{align}
\end{boxexercise}


Let's denote as \citet{FWBQ}
\begin{boxdefinition}
  \begin{align*}
    p[f] &:= \int _ { \X } f ( x ) p ( x ) \mathrm { d } x\\
    \p[f]&:=\sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right).
  \end{align*}
\end{boxdefinition}


We will use the \textit{maximum mean discrepancy} (MMD) as our main metric to measure
 the accuracy of the approximation $p[f] \approx \p[f]$ in the worst case scenario and which is defined as
 \begin{boxdefinition}
    $$\operatorname { MMD } \left( \left\{ x _ { i } , w _ { i } \right\} _ { i = 1 } ^ { n } \right) : = \sup _ { f \in \H : \left[ f \left\| _ { \H } = 1 \right. \right. } | p [ f ] - \hat { p } [ f ] |.$$
 \end{boxdefinition}


% montrer que p et $\p$ sont continues
% théorème de Riesz
% écrire produit scalaire
% on maximise avec cauchy schwartz + égalité

\section{Bayesian Quadrature}
\label{sec:BQ}
% introduction gaussian process
% show that $p[f]$ is also a gaussian process (which expectation and variance)
% Conditional rule demonstrated
% Demonstration formula 4
% Demonstration formula 5

\section{Frank-Wolfe algorithm}
\label{sec:FW}
% intuition
% graph to show that it is closed to a gradient descent
% drawing line search Aspremont

\section*{Ressources}
\label{sec:bibli}
% \nocite{*}
% \bibliographystyle{plain}
% \bibliography{bib_project}
\printbibliography
