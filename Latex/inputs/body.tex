\section*{Notations}
We start with some notations we will use along this report.
\begin{itemize}[font= \color{blue} \large, label= $\bullet$]
 \item For any function, $g(\cdot)$ denotes the function $g: x \mapsto g(x)$.
 \item In integrals, $\d x$ denote $\d \lambda(x)$, \ie with respect to the Lebesgue measure.
\end{itemize}

\section*{Introduction}

The goal of the article \cite{FWBQ} is to compute efficiently the integrals of the form
$ \displaystyle \int _ { \X } f ( x ) p ( x ) \mathrm { d } x$
where $\X \subseteq \R ^ { d }$ is a measurable space,
$d \geq 1$ integer representing the dimension of the problem, $p$ a probability
density with respect to the Lebesgue measure on $\X$ and $f : \X \rightarrow \R$
 is a \textit{test}-function.

 We will use the common approximation
 \begin{boxproblem}
   \begin{equation}
  \int _ { \X } f ( x ) p ( x ) \d x \approx \sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right)
   \end{equation}
 \end{boxproblem}

 but of course the real challenge lies in the choice of sequences $\acc{x_i}$ and
 $\acc{w_i}$ :
  \begin{mydescription}
  \item [Monte Carlo]: $w_i = \frac{1}{n}$ and $x_i$ realization of multivariate random variable $X_i \stackrel{iid}{\sim} X$ where $X$ has $p(\cdot)$ as probability distribution.
  \item [Quasi-Monte Carlo]:$w_i = \frac{1}{n}$ and $x_i$ are chosen according to a low-discrepancy sequence. The Quasi-Monte Carlo method converges with rate $\O\prt{\frac{1}{ n}}$ whereas the classical Monte Carlo method converges with rate $\O \prt{\frac{1}{\sqrt { n }}}$.
 \item [Kernel herding]: the sequences $(w_i)$ and $(x_i)$ are updated according to the following equations : $x_{t+1} = \underset { x \in \chi}{ \argmax } \left< { w }_{ t },\Phi (x) \right> $ and $w_{t+1} = { w }_{ t }+{ \Expec{} }_{ x\sim p }\left[ \Phi (x) \right] -\Phi ({ x }_{ t+1 })$ where $\Phi(\cdot)$ denotes the feature map function from $\X$ to an Hilbert space that we will later introduce. This method has the same rate of convergence than the Quasi-Monte Carlo method.
  \item [Frank-Wolfe Bayesian Quadrature]:
  \begin{myitemize}[0.2cm]
\item $\acc{w_i}$ appear naturally in the Bayesian Quadrature by taking the expectation of a posterior distribution  (described in section \ref{sec:BQ}),
\item $\acc{x_i}$ are selected by the Frank-Wolfe algorithm in order to minimize a posterior variance (described in section \ref{sec:FW}).
\end{myitemize}
  \end{mydescription}
  The main interest of the method developed in \cite{FWBQ} is the super fast
  \textit{exponential} convergence to the true value of the integral compared to the other methods mentioned above.\\

\noindent
  Through this report, we will detail every results from \cite{FWBQ} with the goal
  to clarify and explain details that could have been omitted intentionally or not and which, in our view, make the Briol's and al. approach more natural, intuitive and easier
  to understand.\\

  % use Bach presentation for convergence rates benchmark
  \noindent
  Lastly, we were able to successfully reproduce all the simulations from \cite{FWBQ} with an additional application in pricing of financial derivatives with real worlds dataset. The full code can be found here :
\begin{center}
  \url{https://github.com/ncloarec/FWBQ_notebooks}
\end{center}


\section{Background}
% write $\mu_p$ with an integral
%WHY RKHS, WHY so USEFUL ?
Let $\X \subseteq \R ^ { d }$ be a measurable space, $\mu$ a measure on $\X$ such
that $p = \frac{\d \mu}{ \d \lambda}$ where $\lambda$ denotes the Lebesgue measure on $\X$,
  $\H \subset L^2(\X, \R; \mu)$ be an RKHS with a
 reproducing kernel $k: \X \times \X \rightarrow \R$, $\Phi$ its canonical feature
 map associated. We denote respectively by $\bigps{\cdot}{\cdot}$ and $\norme{\cdot}$
 the bigps product and norm induced on $\H$.

 Recall that the following relations hold:
 \begin{boxexercise}
   \begin{align}
  \forall x \in \X , \quad &k ( \cdot , x ) \in \H\\
  \forall x \in \X , \forall f \in \H , \quad &\langle f , k ( \cdot , x ) \rangle _ { \H } = f ( x )\\
  \forall (x,y) \in \X^2 \quad &k(x,y) = \bigps{\Phi(x)}{\Phi(y)}
  \end{align}
\end{boxexercise}


Let's denote as \cite{FWBQ}:
\begin{boxdefinition}
  \begin{align*}
    p[f] &:= \int _ { \X } f ( x ) \mathrm { d } \mu(x) = \int _ { \X } f ( x ) p ( x ) \mathrm { d } x\\
    \p[f]&:=\sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right).
  \end{align*}
\end{boxdefinition}


We will use the \textit{maximum mean discrepancy} (MMD) as our main metric to measure
 the accuracy of the approximation $p[f] \approx \p[f]$ in the worst case scenario and which is defined as
 \begin{boxdefinition}
    $$\MMD : = \sup _ { f \in \H : \norme{f}=1 } | p [ f ] - \hat { p } [ f ] |.$$
 \end{boxdefinition}

 Let's show (formula 3. in \cite{FWBQ}) that MMD can be rewrite as
 \begin{boxtheorem}
   \begin{align}
     \label{eq:MMD}
     \MMD = \left\| \mu _ { p } - \mu _ { \hat { p } } \right\| _ { \mathcal { H } }
   \end{align}
   where $\mu_p(\cdot) = p[\Phi(\cdot)]$ and $\mu_{\p}(\cdot) = \p[\Phi(\cdot)]$.
 \end{boxtheorem}

   \begin{itemize}[leftmargin=*, font= \color{blue} \large, label= $\bullet$]
     \item \textbf{For all $f$ in $\H$, we have $p[f] = \bigps{f}{\mu_p}$.} By using
     the dirac delta function, the continuity of the inner product and viewing integral
     as a limit of a sum, we get
     \begin{boxcomputation}
     \begin{align*}
       p[f] &= \int_{\X} f(x) \d \mu(x)\\
       &= \int_{\X} \delta_x[f]  \d \mu(x)\\
       &= \int_{\X} \bigps{f}{\Phi(x)}  \d \mu(x)\\%TODO Not obvious
       &=  \bigps{f}{\int_{\X} \Phi(x) \d \mu(x)} \\
       &= \bigps{f}{\mu_p}
     \end{align*}
      \end{boxcomputation}
     \item \textbf{For all $f$ in $\H$, we have $\p[f] = \bigps{f}{\mu_{\p}}$.}
     \begin{boxcomputation}
     \begin{align*}
       \p[f] &= \sum_{i=1}^n w_i f(x_i)\\
       &= \sum_{i=1}^n w_i \delta_{x_i}[f]\\
       &= \sum_{i=1}^n w_i \bigps{f}{\Phi(x_i)}\\%TODO Not obvious
       &=  \bigps{f}{\sum_{i=1}^n w_i \Phi(x_i)} \\
       &= \bigps{f}{\mu_{\p}}
     \end{align*}
   \end{boxcomputation}
\item By using previous results and the Cauchy-schwartz inequality, we get :
\begin{boxcomputation}
\begin{align*}
\MMD &= \sup _ { f \in \H : \norme{f} = 1 } \abs{
\bigps{f}{\mu_{p}-\mu_{\p}} }\\
&\leq \sup _ { f \in \H : \norme{f} = 1 } \norme{f} \norme{\mu_{p}-\mu_{\p}}\\
&= \norme{\mu_{p}-\mu_{\p}}
\end{align*}
 \end{boxcomputation}
with equality if and only if $f$ and  $\mu_{p}-\mu_{\p}$ are linearly dependent.
We deduce the desired result by taking $ \displaystyle f = \frac{1}{\norme{\mu_{p}-\mu_{\p}}}\prt{\mu_{p}-\mu_{\p}}$.

   \end{itemize}



% on maximise avec cauchy schwartz + égalité

\section{Bayesian Quadrature}
\label{sec:BQ}
Let's place a functional prior on the integrand $f$ and denote by $(\Omega, \mathcal{F}, \P)$ its probability space associated. We will assume that $f$ to be
 a \textit{centered} \textbf{gaussian process} with the kernel $k$ as its covariance
 function, \ie
 \begin{boxexample}
   \begin{align*}
     \forall x \in \H, \quad  &\Expec{f(x)} = 0\\
     \forall x, y \in \H, \quad  &\Cov{f(x)}{f(y)} = k(x,y)
   \end{align*}
 \end{boxexample}
A useful property is that $p[f]$ is a gaussian variable and then completely defined by its second-order statistics:
\begin{boxtheorem}
  \begin{align}
    \Expec{p[f]} &= 0\\
    \Var{p[f]} &= \int_{\X^2} k(x,y) \d \mu(x) \d \mu(y)
  \end{align}
\end{boxtheorem}


  By switching integrals using Fubini's theorem, we get
  \begin{boxcomputation}
    \begin{align*}
      \Expec{p[f]} &= \int_{\Omega} p[f](w) \d \P(w)\\
      &= \int_{\Omega} \int_{\X} f(x,w) \d \mu(x)  \d \P(w)\\
      &= \int_{\X} \underbrace{\int_{\Omega}  f(x,w) \d \P(w)}_{\Expec{f(x)}=0} \d \mu(x)  =0
    \end{align*}
\end{boxcomputation}
\begin{boxcomputation}
  \begin{align*}
      \Var{p[f]} &= \Expec{p[f]^2} = \int_{\Omega} p[f](w)^2 \d \P(w)\\
      &=\int_{\Omega} \prt{\int_{\X} f(x,w) \d \mu(x)}^2 \d \P(w)\\
      &=\int_{\Omega} \int_{\X^2} f(x,w)f(y,w) \d \mu(x) \d \mu(y) \d \P(w)\\
      &= \int_{\X^2} \underbrace{\int_{\Omega}  f(x,w)f(y,w) \d \P(w)}_{=\Cov{f(x)}{f(y)} = k(x,y)} \d \mu(x) \d \mu(y) \\
      &= \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y)
    \end{align*}
\end{boxcomputation}

Assume that samples $\acc{x_i}$ and $\acc{f_i} := \acc{f(x_i)}$ are given for $i=1$ to $n$ and denote by $K:= \prt{k(x_i, x_j)}_{1 \leq i,j \leq n}$. A natural question arises: how to update the weights $\acc{w_i}_{i=1}^n $?

\begin{figure}[H]
\centering
\includegraphics[scale=.33]{bq.png}
\caption{\textbf{An illustration of Bayesian Quadrature.}\\\textit{Source:} \cite{huszar}}
\label{fig:bq}
\end{figure}

First of all, let's determine the conditional distribution $\displaystyle
 p[f]| \; \mathbf{f}$ where
$\mathbf{f} = \transpose{\prt{f_1, \ldots, f_n}}$.
Since both $p[f]$ and $\mathbf{f}$ are gaussian, we can use the conditional gaussian rule:
\begin{boxdefinition}
  By denoting, $y_1 := p[f]$, $y_2 = \mathbf{f}$, $y = \begin{pmatrix}
    y_1\\y_2
\end{pmatrix}$, $\displaystyle \Sigma = \begin{pmatrix}
  \Sigma_{1,1}, \Sigma_{1,2}\\
  \Sigma_{2,1}, \Sigma_{2,2}
\end{pmatrix}$ its covariance matrix by blocks, we have :
$$p[f]| \; \mathbf{f} \sim \mathcal { N } (  \mu  , \Stilde )$$
where

  %
  %

  \begin{equation*}
  \begin{cases}
    \mu \;&= \;  \Sigma _ { 12 } \; \Sigma _ { 22 } ^ { - 1 } \; \mathbf{f} \\
     \Stilde \; &= \;  \Sigma _ { 11 } \;- \Sigma _ { 12 } \;\Sigma _ { 22 } ^ { - 1 } \; \Sigma _ { 21 }.
  \end{cases}
\end{equation*}
\end{boxdefinition}

Let's determine what $\mu$ and $\Stilde$ look like in our context.
  \begin{boxcomputation}
\begin{align*}
  \Sigma _ { 22 } &= \prt{\Cov{f_i}{f_j}}_{1 \leq i,j \leq n}\\
  &= \prt{\Cov{f(x_i)}{f(x_j)}}_{1 \leq i,j \leq n}\\
  &= \prt{k(x_i, x_j)}_{1 \leq i,j \leq n}\\
  &= K\\
  \mu &= \begin{pmatrix}
    \Cov{p[f]}{f_1} \\ \ldots \\ \Cov{p[f]}{f_n}\\
\end{pmatrix} K^{-1} \mathbf{f}
\end{align*}
\end{boxcomputation}
Let's rewrite the vector from the left:
\begin{boxcomputation}
\begin{align*}
\Cov{p[f]}{f_i} &= \int_{\Omega} p[f](w) f(x_i, w) \d \P(w)\\
&= \int_{\Omega} \int_{\X} f(x, w) \d \mu(x) f(x_i, w) \d \P(w)\\
&=  \int_{\X}  \underbrace{\int_{\Omega} f(x, w) f(x_i, w) \d \P(w)}_{=\Cov{f(x)}{f(x_i)}=k(x,x_i)} \d \mu(x) \\
&= \int_{\X} k(x,x_i)\d \mu(x)\\
&= \int_{\X} \Phi(x_i)(x) \d \mu(x)\\
&= p[\Phi(x_i)]\\
&= \mu_p \prt{x_i}\\
\end{align*}
\end{boxcomputation}

By denoting $z := \prt{z_i}_{i=1}^n = \prt{\mu_p \prt{x_i}}_{i=1}^n \in \R^n$, we
 get the desired formula\footnote{formula 4 in \cite{FWBQ} } for the expectation:
 \begin{boxtheorem}
   \begin{equation}
     \mu = z ^ { T } K ^ { - 1 } \mathbf { f }
   \end{equation}
 \end{boxtheorem}

The variance is then straightforward :
\begin{boxexample}
  \begin{align*}
    \Stilde &= \Var{p[f]} - z ^ { T } K ^ { - 1 } z\\
    &= \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y) - z ^ { T } K ^ { - 1 } z
  \end{align*}
\end{boxexample}

\begin{boxcomputation}
  We need to simplify the integral to obtain the desired formula for the variance:
\begin{align*}
  \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y) &= \int_{\X} \int_{\X} \Phi(x)(y) \d \mu(y) \d \mu(x) \\
  &= \int_{\X} p[\Phi(x)] \d \mu(x) \\
  &= \int_{\X} \mu_p\prt{x} \d \mu(x) \\
  &=  p[\mu_p]
\end{align*}
\end{boxcomputation}
which leads us to the desired result:
\begin{boxtheorem}
  $$\Stilde = p[\mu_p] -z ^ { T } K ^ { - 1 } z$$
\end{boxtheorem}

Both the conditional expectation and variance are essentials in the FWBQ\footnote{Frank-Wolfe Bayesian Quadrature} algorithm:
\begin{itemize}[font= \color{blue} \large, label= $\bullet$]
  \item $\mu = z ^ { T } K ^ { - 1 } \mathbf { f }$ which can be written as
  $\displaystyle \mu = \sum_{i=1}^n w_i^{BQ} f_i$ where $w^{BQ} := \transpose{\prt{K^{-1}}}z$. $\mu$ appears to be the \textbf{most natural estimation of our integral} $p[f]$.
  It also gives us the \textbf{new weights to update}.
  \item $\Stilde = p[\mu_p] -z ^ { T } K ^ { - 1 } z$ which is also equal to $\MMD^2$
  according to \cite{huszar} and can be interpreted as our \textbf{uncertainty}. Therefore
  we need to \textbf{choose $\acc{x_i}$ which minimize this quantity}. This will be achieved
  by using the Frank Wolfe algorithm described in the next section.
\end{itemize}

% Conditional rule demonstrated
% Demonstration formula 4
% Demonstration formula 5

\section{Frank-Wolfe algorithm}
\label{sec:FW}

Let's $J$ be a convex differentiable real-valued function on a
domain $\G\subset\H$ which is supposed to be a compact and convex.

The Frank-Wolfe algorithm \cite{fwalgo} proposes a method to solve the following optimization
 problem :

\begin{boxproblem}
  \begin{equation} \label{eq:fwpbm}
    \begin{array} { l } { \text { Minimize } \acc{J ( \mathbf { x } )} } \\ { \text { subject to } \mathbf { x } \in \G } \end{array}
  \end{equation}
\end{boxproblem}
Let's describe here both the Frank-Wolfe algorithm and its variant with the Line Search :
\begin{boxprogramming}
\begin{mydescription}
  \item [Initialization] $g _ { 1 } = \overline { g } _ { 1 } \in \mathcal { G }$ and
  step-size sequence $\acc{\rho_i}_{i=1}^n$ (not required in the FWLS algorithm).
  \item [Iterations] For $i=2$ to $n$ :
  \begin{mydescription}[0.2cm]
    \item [Search direction] we replace in \eqref{eq:fwpbm} $J$ by its first-order
    Taylor expansion around $g_k$ to solve the following subproblem :
      \begin{equation}\label{eq:fwlin}
        \begin{array} { c } { \text { minimize }  \acc{J \left( g_{i-1} \right) + \nabla J \left( g_{i-1} \right) ^ { \mathrm { T } } \left( g - g_{i-1} \right)} } \\ { \text { subject to } g \in \G } \end{array}
      \end{equation}
Let's denote as \cite{FWBQ}
\begin{equation}\label{eq:gbar}
  \overline { g } _ { i } := \arg \min _ { g \in \G } \bigps{g}{ \nabla J \left( g _ { i - 1 } \right)}
\end{equation}
 where we have removed
 in \eqref{eq:fwlin} terms independent of the optimization variable.
 \item [New iteration point]: we choose $g_i$ as a \textit{convex}-combination of
 $g_{i-1}$ and $\overline{g_i}$.
 \begin{equation}
   \label{eq:12}
   g _ { i } = \left( 1 - \rho _ { i } \right) g _ { i - 1 } + \rho _ { i } \overline{ g } _ { i }
 \end{equation}
 \begin{mydescription}
\item [FW] $\rho _ { i }$ is  determined by the sequence given at the initialization
\item [FWLS]
\begin{equation*}
\rho _ { i } := \operatorname { argmin } _ { \rho \in [ 0,1 ] } J \left( ( 1 - \rho ) g _ { i - 1 } + \rho \overline{g _ { i }} \right)
\end{equation*}

 \end{mydescription}
  \end{mydescription}
\end{mydescription}
\end{boxprogramming}

\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-9,-4) rectangle (0.24,2);
\fill[line width=2pt,color=ggyellow,fill=ggyellow,fill opacity=0.10000000149011612] (-5.58,1.24) -- (-3.52,1.34) -- (-1.08,-1.24) -- (-4.38,-3.46) -- (-8.1,-2.2) -- cycle;
\draw [color=ggyellow] (-5.58,1.24)-- (-3.52,1.34);
\draw [color=ggyellow] (-3.52,1.34)-- (-1.08,-1.24);
\draw [color=ggyellow] (-1.08,-1.24)-- (-4.38,-3.46);
\draw [color=ggyellow] (-4.38,-3.46)-- (-8.1,-2.2);
\draw [color=ggyellow] (-8.1,-2.2)-- (-5.58,1.24);
\draw [->] (-4.42,-0.94) -- (-3.02,-0.1);
\draw [dotted] (-4.42,-0.94)-- (-7.29,-2.47);
\draw [dotted] (-4.42,-0.94)-- (-8.1,-2.2);
\draw [->] (-5.73,-1.38) -- (-4.42,-1.84);
\draw [dotted] (-5.41,-1.47)-- (-7.10,-0.84);
  \node[draw] at (-3.02,0.3)  (grad1) {$\mathbf{\nabla J(g_1)}$};
  \node[draw] at (-4.22,-2.3)  (grad2) {$\mathbf{\nabla J(g_2)}$};
\draw [fill=blue] (-4.42,-0.94) circle (2pt) node[below] {$\mathbf{g_1}$};
\draw [fill=blue] (-8.1,-2.2) circle (2pt) node[shift={(-1.5ex,-2ex)}] {$\mathbf{\overline{g_2}=\overline{g_3}}$};
\draw [fill=blue] (-5.73,-1.38) circle (2pt) node[shift={(0ex,-2.5ex)}] {$\mathbf{g_2}$};
\draw [fill=blue] (-6.32,-1.59) circle (2pt) node[shift={(-0.5ex,2ex)}] {$\mathbf{g_3}$};
\draw[->,ggred, looseness=2] (-4.42,-0.94) .. controls (0,-2.3) and  (-4,-4.5) .. node[sloped] {} (-8.1,-2.2);
\draw[->,ggred] (-8.1,-2.2) -- (-5.73,-1.38);
\draw[->,ggred, looseness=2] (-5.73,-1.38) .. controls (-5.6,0.5) .. node[sloped] {} (-8.1,-2.2);
\draw[->,ggred] (-8.1,-2.2) -- (-6.32,-1.59);
\end{tikzpicture}
\begin{center}
  \textbf{An illustration of the Frank Wolfe algorithm. }
\end{center}
  In equation \eqref{eq:gbar}, we don't have necessarily $\overline{g_i} \in \spn{\nabla J(g_{i-1})}$. In fact, we can clearly see in the illustration above
   that the point
   \begin{align*}
  \widetilde { g } _ { i } &:= \arg \min _ { g \in \G\cap  \spn{\nabla J(g_{i-1})}} \bigps{g}{ \nabla J \left( g _ { i - 1 } \right)} \\
  &= \arg \min _ { \alpha \geq 0}
  \acc{-\alpha \nabla J(g_{i-1}) \mid \alpha \nabla J(g_{i-1}) \in \G}
\end{align*}
would not be optimal as
\begin{equation*}
  \inner{\overline { g } _ { i }}{\nabla J(g_{i-1})}
  < \inner{\widetilde { g } _ { i }}{\nabla J(g_{i-1})}.
\end{equation*}



% \hspace{-1.5cm}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-9,-4) rectangle (0.24,2);
\fill[line width=2pt,color=ggyellow,fill=ggyellow,fill opacity=0.10000000149011612] (-5.58,1.24) -- (-3.52,1.34) -- (-1.08,-1.24) -- (-4.38,-3.46) -- (-8.1,-2.2) -- cycle;
\draw [color=ggyellow] (-5.58,1.24)-- (-3.52,1.34);
\draw [color=ggyellow] (-3.52,1.34)-- (-1.08,-1.24);
\draw [color=ggyellow] (-1.08,-1.24)-- (-4.38,-3.46);
\draw [color=ggyellow] (-4.38,-3.46)-- (-8.1,-2.2);
\draw [color=ggyellow] (-8.1,-2.2)-- (-5.58,1.24);
\draw [->] (-4.42,-0.94) -- (-3.02,-0.1);
\draw [dotted] (-4.42,-0.94)-- (-7.29,-2.47);
\draw [->] (-5.41,-1.47) -- (-4.42,-1.84);
\draw [dotted] (-5.41,-1.47)-- (-7.10,-0.84);
  \node[draw] at (-3.02,0.3)  (grad1) {$\mathbf{\nabla J(g_1)}$};
  \node[draw] at (-4.22,-2.3)  (grad2) {$\mathbf{\nabla J(g_2)}$};
\draw [fill=blue] (-4.42,-0.94) circle (2pt) node[below] {$\mathbf{g_1}$};
\draw [fill=blue] (-7.294,-2.47) circle (2pt) node[shift={(-1.5ex,-1.5ex)}] {$\mathbf{\widetilde{g_2}}$};
\draw [fill=blue] (-5.41,-1.47) circle (2pt) node[shift={(0ex,-2.5ex)}] {$\mathbf{g_2}$};
\draw [fill=blue] (-7.10,-0.846) circle (2pt) node[shift={(-1.5ex,1.5ex)}] {$\mathbf{\widetilde{g_3}}$};
\draw [fill=blue] (-6.01,-1.246) circle (2pt) node[shift={(0ex,2.5ex)}] {$\mathbf{g_3}$};
\draw[->,ggred, looseness=2] (-4.42,-0.94) .. controls (0,-2.3) and  (-4,-4.5) .. node[sloped] {} (-7.29,-2.47);
\draw[->,ggred] (-7.29,-2.47) -- (-5.41,-1.47);
\draw[->,ggred, looseness=2] (-5.41,-1.47) .. controls (-5.6,0.5) .. node[sloped] {} (-7.10,-0.846);
\draw[->,ggred] (-7.10,-0.846) -- (-6.01,-1.246);
\end{tikzpicture}
\begin{center}
  \textbf{Comparison with a descent algorithm. }
\end{center}

Another important fact of the Frank Wolfe algorithm is that the output $g_n$
can easily be expressed as a linear combination of atoms $\overline{g_i}$.

In fact we have the following formula, which corresponds to the equation (\textcolor{electriccrimson}{7}) in \cite{FWBQ}:
\begin{boxtheorem}
  For all $n \geq 2$ :
  \begin{equation}
    \label{eq:13}
    g_n = \sum_{k=1}^n \underbrace{ \rho_k \acc{\prod_{k<i\leq n} (1-\rho_i)} }_{:=w_k^{FW}} \cdot \overline{g_k} = \sum_{k=1}^n w_k^{FW}\cdot \overline{g_k}
  \end{equation}
  with $\rho_1 = 1$.
\end{boxtheorem}

The formula in \cite{FWBQ} is slightly different :
\begin{equation*}
  g _ { n } = \sum _ { i = 1 } ^ { n } \rho _ { i - 1 }\acc{ \prod _ { j = i + 1 } ^ { n } \left( 1 - \rho _ { j - 1 } \right) } \cdot \overline { g } _ { i }
\end{equation*}
with $\rho_0=1$ but clearly doesn't work. In fact according to the algorithm, by taking $n=2$ we should have :
\begin{equation*}
  g_2 = (1-\rho_2) \cdot \overline{g_1} + \rho_2 \cdot \overline{g_2}
\end{equation*}
while their formula gives
\begin{align*}
  g _ { 2 } &= \sum _ { i = 1 } ^ { 2 } \rho _ { i - 1 }\acc{ \prod _ { j = i + 1 } ^ { 2 } \left( 1 - \rho _ { j - 1 } \right) } \overline { g } _ { i }\\
  &= (1-\rho_1) \cdot  \overline{g_1}+\rho_1 \cdot \overline{g_2}.
\end{align*}
Let's prove \eqref{eq:13} by induction.

\begin{boxcomputation}
\begin{mydescription}
  \item [Base case]: $n=2$. Our formula gives :
  \begin{align*}
    g_2 &= \sum_{k=1}^2  \rho_k \acc{\prod_{k<i\leq 2} (1-\rho_i)} \cdot \overline{g_k}\\
    &= (1-\rho_2) \cdot \overline{g_1}+ \rho_2 \cdot \overline{g_2}
  \end{align*}
  as we have set $\rho_1 = 1$, as expected.
  \item [Step case]: Assume the formula holds for $n-1$ and let's prove it for $n$.
  Using the recurrence relation of \eqref{eq:12} and our assumption, we have :
  \begin{align*}
    g _ {n} &= \left( 1 - \rho _ {n} \right) \cdot g _ {n- 1 } + \rho _ {n} \cdot \overline { g } _ {n}\\
    &= \left( 1 - \rho _ { n } \right) \cdot \sum_{k=1}^n \rho_k \acc{\prod_{k<i\leq n-1} (1-\rho_i)} \cdot \overline{g_k} + \rho_n \cdot \overline { g } _ {n}\\
    &= \sum_{k=1}^n \rho_k \acc{\prod_{k<i\leq n} (1-\rho_i)} \cdot \overline{g_k} + \rho_n\cdot \overline { g } _ {n}\\
    &= \sum_{k=1}^n \rho_k \acc{\prod_{k<i\leq n} (1-\rho_i)}  \cdot \overline{g_k}
  \end{align*}
  which ends the proof.
\end{mydescription}
\end{boxcomputation}

It is common to take harmonic coefficients in the standard FW algorithm\footnote{Let's remember that in the FWLS algorithm, $\acc{\rho_i}$ and thus $\acc{w_k^{FWLS}}$ are
determined by the line search step.} as a choice for $\acc{\rho_i}:= \acc{\frac{1}{i}}_{i=1}^n$,
 which gives us uniform weights as showed bellow :
 \begin{align*}
w_k^{FW} &= \frac{1}{k} \acc{\prod_{k<i\leq n} \prt{1-\frac{1}{i}}}= \frac{1}{k} \cdot \frac{k}{n} = \frac{1}{n}.
\end{align*}

Let's set here the framework of our optimization problem.

\begin{boxproblem}
  We want to approximate the mean element $\mu_p$ with a linear combination of
  elements of the form :$$\Phi(x)= k\prt{\cdot\;,\; x}.$$
  In fact, let's say we have:
   $$\mu_p \approx \hat{\mu_p} := \sum_{k=1}^n w_k \Phi(x_k).$$
   Using the reproducing property, we will then have :
   $$\hat{p}[f]= \bigps{f}{\mu_p} = \sum_{k=1}^n w_k f(x_k),$$
   which is a quadrature rule.

   In order to do this, let's assume that we optimize the following convex function :
   \begin{equation}
     J(g):= \frac{1}{2} \norme{g-\mu_p}^2
   \end{equation}
on the domain $\G \subseteq \H$ where $\G$ refers to the closure of the convex hull
of $\Phi\prt{\X}$, which is assumed to be uniformly bounded, \ie
\begin{equation*}
  \exists R > 0 : \forall x \in \mathcal { X } , \| \Phi ( x ) \| _ { \mathcal { H } } \leq R
\end{equation*}

\end{boxproblem}
But a question rises : Why atoms $g_i$ should be of the form $\Phi(x_i)$?

In fact, let's
 $g$ be a point in the domain. Because of its definition,
  $g$ can simply be expressed as a convex combination of elements in the feature space $\Phi(\X)$, \ie $g = \sum_k \alpha_k \Phi(x_k)$ with $\sum_k \alpha_k=1$ and thus we have :
  \begin{boxcomputation}
    \begin{align*}
      \bigps{g}{\grad{J}(g_{i-1})} &= \sum_k \alpha_k \cdot \bigps{\Phi(x_k)}{\grad{J}(g_{i-1})}\\
      &\geq \prt{\sum_k \alpha_k} \cdot \min_{k} \bigps{\Phi(x_k)}{\grad{J}(g_{i-1})}\\
      &= \bigps{\Phi(x_{k_0})}{\grad{J}(g_{i-1})}
    \end{align*}
    for $k_0$ such that $k_0 \in \argmin{\bigps{\Phi(x_k)}{\grad{J}(g_{i-1})}}$.
  \end{boxcomputation}


 We now understand how can the Frank Wolfe algorithm be useful here : because the
 minimization of \eqref{eq:gbar} can be restricted at extreme points of the domain,
 selecting atoms of the form $\Phi(x_k)$ allow us to select interesting points $\acc{x_k}$ which are needed for the bayesian quadrature which are solution of the
 following optimization problem :
 \begin{equation}\label{eq:xmin}
   x_k \in \arg \min _ { x \in \mathcal { X } } \left\langle \Phi ( x ) , g _ { k - 1 } - \mu _ { p } \right\rangle _ { \mathcal { H } }
 \end{equation}
 where $g \in \G$ in the left term of the inner product has been replaced by $\Phi(x) \in \G$ with $x \in \X$.

 Therefore, if we denote by $\acc{w_l}_{l=1}^{i-1}$ the coefficients in front of
 $\acc{\Phi(x_l)}_{l=1}^{i-1}$ in $g_{i-1}$ and using the reproducing kernel property, we have :
 \begin{boxcomputation}
   \begin{align*}
     \left\langle \Phi ( x ) , g _ { i - 1 } - \mu _ { p } \right\rangle _ { \mathcal { H } } &= \left\langle \Phi ( x ) , \sum _ { l = 1 } ^ { i - 1 } w _ { l } ^ { ( i - 1 ) } \Phi \left( x _ { l } \right) - \mu _ { p } \right\rangle _ { \mathcal { H } }\\
     &= \sum _ { l = 1 } ^ { i - 1 } w _ { l } ^ { ( i - 1 ) } k \left( x , x _ { l } \right) - \mu _ { p } ( x )\\
   \end{align*}
 \end{boxcomputation}
which gives an explicit formula for the optimization problem\footnote{We remark that if
 the optimization problem on $g \in \G$ was a classic convex problem, this is not the case anymore when we perform the optimization on $x \in \X$.} with known quantities.
 In fact, in the simulations that we have been able to reproduce successfully, the choice of a gaussian kernel $k$ allow us to compute $\mu_p( \cdot )$ easily. See \cite{FWBQ} \textit{Appendix C} for more details.
 \section{The Frank-Wolfe Bayesian Quadrature algorithm}
 \label{sec:CC}

We describe here the final algorithm which is a combination of the two previous
 ones.
    At each iteration \texttt{i}:
 \begin{mydescription}
   \item [Selecting a new $x_i$] through the Frank Wolfe algorithm with $g_{i-1}$ depending
   on the BQ weights of the iteration \texttt{i-1} and FW design points of all previous iteration \texttt{j} with $1 \leq j<i$.
   \item [Selecting new weights $\acc{w_k^{BQ}}_{k=1}^i$] with the bayesian quadrature.
 \end{mydescription}

 It's important to note that weights at each iteration \texttt{i} are used only for
 the next iteration \texttt{i+1} while the design points $x_i$ are used for all iteration \texttt{j} with $i<j\leq n$.

\section{Consistency}
\label{sec:consistency}

We will establish here the main result of the article \cite{FWBQ} and referred as Theorem 1:
\begin{boxdefinition}
  \begin{center}
    \textit{The posterior mean }$\hat { p } _ { \mathrm { FWBQ } } [ f ]$ \textit{converges
    to the true integral }$p[f]$ at the following rates:
  \end{center}
\begin{align}
    \left| p [ f ] - \hat { p } _ { \mathrm { FWBQ } } [ f ] \right| &\leq M M D \left( \left\{ x _ { i } , w _ { i } \right\} _ { i = 1 } ^ { n } \right) \nonumber\\
    &\leq \left\{ \begin{array} { c c } { \frac { 2 D ^ { 2 } } { R } n ^ { - 1 } } & { \footnotesize \text {for  FWBQ} } \\ { \sqrt { 2 } D \exp \left( - \frac { R ^ { 2 } } { 2 D ^ { 2 } } n \right) } & {\footnotesize \text {for FWLSBQ } } \end{array} \right.
\end{align}
where the FWBQ uses step-size $\rho _ { i } = 1 / ( i + 1 )$, $D \in ( 0 , \infty )$
  is the diameter of the marginal polytope $\G$ and $R \in ( 0 , \infty )$ gives the
  radius of the \textbf{largest}\footnote{There was a typo here in \cite{FWBQ} as the authors mentioned the radius  of the smallest ball which is clearly zero. In fact, greater is the radius and better is the inequality, so of course we are more interested in large
  radius rather than smaller ones. } ball of center $\mu_p$ included in $\G$.

  We will suppose $n$ iterations in the FW algorithm (from $i=2$ to $n+1$) instead of $n-1$ (from $i=2$
  to $n$) otherwise one should replace $n$ in the rates above by $n-1$\footnote{It was not necessarily a good idea to initialize the algorithm at $i=1$ for the authors of
  \cite{FWBQ} while all authors of \cite{Bach}, \cite{Beck} and \cite{Chen} have
  decided to initialize at $i=0$. Despite that we have decided to keep their notations
  in order to be consistent with the results in \cite{FWBQ}.
  }.
\end{boxdefinition}

It was a bit disappointing to see that the authors of \cite{FWBQ} did not prove the most interesting part of the theorem,
 which is the introduction of the diameter of the marginal polytope ($D$) and the radius of the largest ball of center $\mu_p$ ($R$). Instead, it came out of nowhere quoting \cite{Bach}, which is of course deeply unsatisfactory from a scientific standpoint
  as it gives no clear understanding about the intuition behind this result.

  We were able to find more details about this result in \cite{Beck} and \cite{Chen}.
  We will focus here on the proof of the FWLSBQ's consistency as it is the most
  difficult one involving few other results and propositions that we will also
  explore. The proof of the FWBQ's consistency in the case of uniform weights $\prt{\rho_i=\frac{1}{i+1}}$ can be adapted from \cite{Chen} proposition 1 when they
  show with same arguments that we will expose here that $w_n := n\cdot
  \prt{\mu_p-g_n}$ is bounded.

\subsection*{Frank Wolfe Bayesian Quadrature with line search}

Let's start assuming as \cite{Bach} that $\mu_p$ is in the relative interior
of $\G$ :
\begin{equation}
\exists r>0 \text{ such that } B(\mu_p, r) \subset \G.
\end{equation}

{$\bullet$~\normalfont\bfseries\color{myorange!90!black} Step 1 }We start with a proposition\footnote{Proposition 3.1 in \cite{Beck}.} :
\begin{boxtheorem}
  We denote by $R$ the radius of the largest ball of center $\mu_p$.
  Using the same notations as previously and the assumption above, we have at
  iteration \texttt{i}:
  \begin{equation}
    \label{eq:18}
    \Bigps{\mu_p-g_{i-1}}{\mu_p-\overline{g_{i}}}+R \norme{\mu_p-g_{i-1}} \leq 0
  \end{equation}
\end{boxtheorem}

  By denoting $d = \mu_p-g_{i-1}$ and using the definition of $R$ we have :
\begin{equation*}
  g = \mu_p+R\cdot \frac{d}{\norme{d}} \in \G.
\end{equation*}
Moreover by using the definition of $\overline{g_{i}}$ we get:
  \begin{align*}
\Bigps{\mu_p-g_{i-1}}{\mu_p-\overline{g_{i}}} &=
\Bigps{-\grad{J(g_{i-1})}}{\mu_p-\overline{g_{i}}}\\&\leq
\Bigps{-\grad{J(g_{i-1})}}{\mu_p-g}\\
&\leq -R\cdot\Bigps{\mu_p-g_{i-1}}{\frac{d}{\norme{d}}}\\
&\leq -R \norme{d}
  \end{align*}
  which is the desired result.

{$\bullet$~\normalfont\bfseries\color{myorange!90!black} Step 2 } There is an
 explicit formula for $\rho_i$ at the $\texttt{i}^{th}$ iteration.

 \begin{boxtheorem}
If we denote by $\ropt$ the optimum in the line search at the $\texttt{i}^{th}$
iteration, the following formula holds :
\begin{equation}
\ropt = \frac { \left\langle g _ { i - 1 } - \mu _ { p } , g _ { i - 1 } - \overline{g_i} \right\rangle _ { \mathcal { H } } } { \left\| g _ { i - 1 } - \overline{g_i} \right\| _ { \mathcal { H } } ^ { 2 } }
\end{equation}
 \end{boxtheorem}
  We will show here the proof from \cite{Beck} instead of the one in \cite{FWBQ}
  as it requires less computation and more intuition\footnote{the authors of \cite{FWBQ} do not give a full proof of this result the most important (showing that $\ropt \in \intff{0}{1}$).}. Let
  \begin{equation*}
    f : \left| \begin{array} { c c c } { \intff{0}{1} } & { \longrightarrow } & { \mathbb { R } } \\ { \rho } & { \longmapsto } & { J(\prt{1-\rho}g_{i-1}+\rho{\overline{g_i}}) } \end{array} \right.
\end{equation*}
Because $J$ is quadratic so is $f$ and we can replace $f$ by its quadratic approximation :
\begin{equation*}
  \begin{split}
    f(\rho) = J ( g_{i-1} ) + \rho \langle \overline{g_i}- g_{i-1} , \nabla J ( g_{i-1} ) \rangle \\ + \frac { 1 } { 2 } \rho ^ { 2 } \|  g_{i-1} - \overline{g_i} \| ^ { 2 }
  \end{split}
\end{equation*}
Taking the derivative of this expression, gives :
\begin{equation*}
  \ropt =   \frac { \left\langle g _ { i - 1 } - \mu _ { p } , g _ { i - 1 } - \overline{g_i} \right\rangle _ { \mathcal { H } } } { \left\| g _ { i - 1 } - \overline{g_i} \right\| _ { \mathcal { H } } ^ { 2 } }.
\end{equation*}
which is the desired result but we need to ensure that $\ropt$ is between $0$ and
 $1$.

By definition of $\overline{g_i}$ :
\begin{align*}
  \overline { g } _ { i } &= \arg \min _ { g \in \mathscr { G } } \left( g , \nabla J \left( g _ { i - 1 } \right) \right\rangle _ { \mathscr { H } }\\
  & \arg \min _ { g \in \mathscr { G } } \left( g - g _ { i - 1 } , \nabla J \left( g _ { i - 1 } \right) \right\rangle _ { \mathscr { H } }\\
\end{align*}
Because $g_{i-1} \in \G$\footnote{Remember that $g_{i-1}$ is a convex combination
of elements of $\Phi(\X)$.}, we have
\begin{align*}
  \left\langle \underbrace{g _ { i - 1 } - \mu _ { p }}_{=\grad{J\prt{g_{i-1}}}} , g _ { i - 1 } - \overline{g_i} \right\rangle _ { \mathcal { H } } &\geq
  \left\langle \grad{J\prt{g_{i-1}}} , g _ { i - 1 } -  g _ { i - 1 }\right\rangle _ { \mathcal { H } }\\
  &\geq 0
\end{align*}
so $\ropt \geq 0$.
Using \eqref{eq:18}, we have
\begin{equation}
  \label{eq:neg}
  \left\langle \mu _ { p } - g _ { i -1} , \mu _ { p } - \overline { g _ { i } } \right\rangle _ { \mathscr { H } } \leq 0.
\end{equation}
Thus
\begin{align*}
  &\left\langle g _ { i - 1 } - \mu _ { p } , g _ { i - 1 } - \overline { g _ { i } } \right\rangle _ { \mathscr { H } } \\&\quad =
  \left\langle \mu_p-g_{i-1} , \prt{\mu_p-g_{i-1}} - \prt{\mu_p-\overline{g_i} } \right\rangle_ { \mathscr { H } }\\
  &\quad =\left\| \mu_p-g_{i-1} \right\|_ { \mathscr { H } } ^ { 2 } - \left\langle \mu_p-g_{i-1} , \mu_p-\overline{g_i}  \right\rangle_ { \mathscr { H } }\\
  &\quad \leq \left\| \mu_p-g_{i-1} \right\|_ { \mathscr { H } } ^ { 2 } - \left\langle \mu_p-g_{i-1} , \mu_p-\overline{g_i}  \right\rangle_ { \mathscr { H } } \\&\qquad + \left( \left\| \mu_p-\overline{g_i}  \right\|_ { \mathscr { H } } ^ { 2 } - \underbrace{\left\langle \mu_p-g_{i-1} , \mu_p-\overline{g_i}  \right\rangle_ { \mathscr { H }} }_{\leq 0} \right)\\
  &\quad = \left\| \prt{\mu_p-g_{i-1}} - \prt{\mu_p-\overline{g_i} } \right\|_ { \mathscr { H } } ^ { 2 }\\
  &\quad = \left\| g _ { i - 1 } - \overline{g_i} \right\| _ { \mathcal { H } } ^ { 2 }
\end{align*}
which gives us $\ropt \leq 1$.

 {$\bullet$~\normalfont\bfseries\color{myorange!90!black} Step 3 } We show here
 the linear convergence rate of the algorithm :
 \begin{boxtheorem}
\begin{equation}
  \label{eq:rec}
  \norme{\mu_p-g_i}^2 \leq \prt{1-q^2}\cdot \norme{\mu_p-g_{i-1}}^2
\end{equation}
where $q = \frac{R}{D}>0$.
 \end{boxtheorem}

Using \eqref{eq:12} and the fact that $J$ is equal to its quadratic approximation, we have :
\begin{align*}
  \norme{\mu_p-g_i}^2
  &= \left( \ropt \right) ^ { 2 } \left\| \overline{g_i}-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } \\&+ 2 \ropt \left\langle \mu_p-g_{i-1} , g_{i-1}-\overline{g_i} \right\rangle_{ \mathscr { H }} \\&+ \left\| \mu_p-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 }
\end{align*}
Substituting the value of $\ropt$ from step 2 yields to :
\begin{align*}
&\left\| \mu_p-g_i \right\|_{ \mathscr { H }} ^ { 2 } \\ &\quad= \frac { \left\| \mu_p-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } \left\| \mu_p-\overline{g_i} \right\|_{ \mathscr { H }} ^ { 2 } - \left\langle \mu_p-g_i , \mu_p-\overline{g_i} \right\rangle_{ \mathscr { H }} ^ { 2 } } { \left\| \overline{g_i}-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } }
\end{align*}
We will need the two following inequalities in this third step:
\begin{align}
  \norme{\mu_p-\overline{g_i}}^2 &\leq \norme{\overline{g_i}-g_{i-1}}^2\\
  R^2 \left\| \mu _ { p } - g _ { i - 1 } \right\| _ { \mathscr { H } }^2 &\leq
  \left\langle \mu _ { p } - g _ { i - 1 } , \mu _ { p } - \overline { g _ { i } } \right\rangle _ { \mathscr { H } }^2
\end{align}

The first one can be showed introducing $\mu_p$ in the right term and using
 \eqref{eq:neg}, while the second one comes from \eqref{eq:18} and the fact that
 the square function is decreasing on $\R_{-}$.

We are now able to prove the result :
\begin{align*}
&\left\| \mu_p-g_i \right\|_{ \mathscr { H }} ^ { 2 } \\ &\qquad= \frac { \left\| \mu_p-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } \left\| \mu_p-\overline{g_i} \right\|_{ \mathscr { H }} ^ { 2 } - \left\langle \mu_p-g_i , \mu_p-\overline{g_i} \right\rangle_{ \mathscr { H }} ^ { 2 } } { \left\| \overline{g_i}-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } }\\
&\qquad\leq \frac { \left\| \mu_p-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } \left\| \mu_p-\overline{g_i} \right\|_{ \mathscr { H }} ^ { 2 } - R^2 \left\| \mu _ { p } - g _ { i - 1 } \right\| _ { \mathscr { H } }^2 } { \left\| \overline{g_i}-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } }\\
&\qquad\leq \frac { \left\| \mu_p-g_{i-1} \right\|_{ \mathscr { H }} ^ { 2 } \left\| \mu_p-\overline{g_i} \right\|_{ \mathscr { H }} ^ { 2 } - R^2 \left\| \mu _ { p } - g _ { i - 1 } \right\| _ { \mathscr { H } }^2 } { \norme{\mu_p-\overline{g_i}}^2 }\\
&\qquad \leq \prt{1-\frac{R^2}{\norme{\mu_p-\overline{g_i}}^2}}\cdot \norme{\mu_p-g_{i-1}}^2\\
&\qquad \leq \prt{1-\frac{R^2}{D^2}}\cdot \norme{\mu_p-g_{i-1}}^2
\end{align*}
where the last inequality comes from
\begin{equation*}
  \norme{\mu_p-\overline{g_i}} \leq \sup_{g_1, g_2 \in \G}   \norme{g_1-g_2} :=D
\end{equation*}

  {$\bullet$~\normalfont\bfseries\color{myorange!90!black} Step 4 }
  Using the results from step $3$ , we will show the following inequality

  \begin{boxtheorem}
\begin{equation}
  \norme{\mu_p-\mu_{\hat{p}}}^2 \leq D^2 \cdot \e^{-\frac{R^2 n}{D^2}}
\end{equation}
  \end{boxtheorem}

Remember that we perfom $n$ iterations, so $\mu_{\hat{p}}=g_{n+1}$.
By direct induction and using \eqref{eq:rec}, we have :
\begin{align*}
  \norme{\mu_p-\mu_{\hat{p}}}^2 &= \norme{\mu_p-g_{n+1}}^2 \\
  &\leq \prt{1-\frac{R^2}{D^2}}^n \cdot \norme{\mu_p-g_{1}}^2\\
  &\leq \e^{n \cdot \log{\prt{1-\frac{R^2}{D^2}}}} \cdot D^2\\
  &\leq D^2 \cdot \e^{-\frac{R^2 n}{D^2}}
\end{align*}
where we have used the classic inequality $\log(1+x) \leq x$ for $x>-1$.

  {$\bullet$~\normalfont\bfseries\color{myorange!90!black} Step 5 }
For this last step we show the convergence rate for the FWLSBQ :
\begin{boxtheorem}
  \begin{equation}
    \left| p [ f ] - \hat { p } _ { \mathrm { FWLSBQ } } [ f ] \right| \leq
    \sqrt { 2 } D \exp \left( - \frac { R ^ { 2 } } { 2 D ^ { 2 } } n \right)
  \end{equation}
\end{boxtheorem}

By using the optimality of the weights in bayesian quadrature for $\acc{x_i}$ given,
 we have :
 \begin{align*}
   \operatorname { MMD } \left( \left\{ x _ { i } ^ { \mathrm { FW } } , w _ { i } ^ { \mathrm { BQ } } \right\} _ { i = 1 } ^ { n } \right) &= \inf _ { \mathrm { w } \in \mathbb { R } ^ { n } } \operatorname { MMD } \left( \left\{ x _ { i } ^ { \mathrm { FW } } , w _ { i } \right\} _ { i = 1 } ^ { n } \right) \\
   & \leq \operatorname { MMD } \left( \left\{ x _ { i } ^ { \mathrm { FW } } , w _ { i } ^ { \mathrm { FW } } \right\} _ { i = 1 } ^ { n } \right).
 \end{align*}
Using \eqref{eq:MMD} and the previous step, we have :
\begin{align*}
  \operatorname { MMD } ^ { 2 } \left( \left\{ x _ { i } ^ { \mathrm { FW } } , w _ { i } ^ { \mathrm { FW } } \right\} _ { i = 1 } ^ { n } \right)
  &= 2 \cdot J(g_{n+1}) \\
  &\leq 2 \cdot D ^ { 2 } \cdot e ^ { - \frac { R ^ { 2 } n } { D ^ { 2 } } }
\end{align*}

Moreover, Cauchy schwartz inequality\footnote{referred as the Koksma Hlawka Inequality in this context. \textit{See Lemma 5 in \cite{Chen}}.} yields to
\begin{align*}
  \left| p [ f ] - \hat { p } _ { \mathrm { FWBQ } } [ f ] \right| &\leq \operatorname { MMD } \left( \left\{ x _ { i } ^ { \mathrm { FW } } , w _ { i } ^ { \mathrm { BQ } } \right\} _ { i = 1 } ^ { n } \right) \underbrace{\| f \| _ { \mathcal { H }}
   }_{\leq 1}
\end{align*}
which allow us to conclude.
\section{Contraction}
\label{sec:contraction}

We expose here the second theorem of \cite{FWBQ} which appears to us to be more
 like a corollary of the previous theorem rather than a whole new result. In fact,
 its proof requires very little effort compare to the precedent result.

 \begin{boxtheorem}
Let $S \subseteq \mathbb { R }$ be an open neighbourhood of the true integral
$p[f]$ and let $\gamma = \inf _ { r \in S ^ { C } } | r - p [ f ] | > 0$.

Then the posterior probability mass on $S ^ { c } = \mathbb { R } \backslash S$
vanishes at a rate:
\begin{equation}
  \operatorname { prob } \left( S ^ { c } \right) \leq \left\{ \begin{array} { c c } { \frac { 2 \sqrt { 2 } D ^ { 2 } } { \sqrt { \pi } R \gamma } n ^ { - 1 } \exp \left( - \frac { \gamma ^ { 2 } R ^ { 2 } } { 8 D ^ { 4 } } n ^ { 2 } \right) } & { \text {}  } \\ { \frac { 2 D } { \sqrt { \pi \gamma } } \exp \left( - \frac { R ^ { 2 } } { 2 D ^ { 2 } } n - \frac { \gamma ^ { 2 } } { 2 D ^ { 2 } } \exp \left( \frac { R ^ { 2 } } { 2 D ^ { 2 } } n \right) \right) } & { \text {} } \end{array} \right.
\end{equation}
for FWBQ and FWLSBQ respectively with the same notations used in theorem 1.
 \end{boxtheorem}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[dot/.style={circle,inner sep=1pt,fill,name=#1}]
     \draw[-latex] (-2,0) coordinate (M) -- ++(6,0) node [right] {$\R$};
     \node [dot=A, fill=red, label = $A$] at (-1,0) {};
     % \node [dot=D, fill=red, label = $Y-\tau$] at (0,0) {};
     \node [dot=B, fill=red, label = $B$] at (3,0) {};
     \node [dot = C, fill=red, label =\text{p[f]}] at (1.5,0) {};
     \draw [decorate,decoration={brace,amplitude=8pt, mirror},xshift=0pt,yshift=-2pt] (1.5,0) -- (3,0) node [black,midway,yshift=-.6cm] {\footnotesize $\gamma$ };
     \draw (A) -- (B);
     \draw [decorate,decoration={brace,amplitude=6pt, mirror},xshift=0pt,yshift=-2pt] (-2,0) -- (-1,0) node [black,midway,yshift=-.6cm] {\footnotesize $S^ { c }$ };
     \draw [decorate,decoration={brace,amplitude=6pt, mirror},xshift=0pt,yshift=-2pt] (3,0) -- (4,0) node [black,midway,yshift=-.6cm] {\footnotesize $S^ { c }$ };
    \end{tikzpicture}
    \caption{$A := \argmax_{s\in S^ { c }}\acc{s \leq p[f]}$ and $B := \argmin_{s\in S^ { c }}\acc{s \geq p[f]}$ with extremums reached since $S^ { c }$ is closed.}
    \label{fig:general}
\end{figure}

Let's show the following proposition :
\begin{boxtheorem}
Let $\acc{m_n}\subset \R$ and $\acc{\sigma_n}\subset\R_{+}^{*}$ such that :
\begin{equation*}
  \begin{cases}
    m_n \rightarrow \mopt\\
    \sigma_n \rightarrow 0  \\
    \frac{\abs{m_n-\mopt}}{\sigma_n} = \O_{n\rightarrow \infty}\prt{1}
\end{cases}
\end{equation*}
with $\mopt$ a real number. Let $S \subseteq \mathbb { R }$ be an open neighbourhood of
$\mopt$, $\gamma = \inf _ { r \in S ^ { C } } | r - \mopt | > 0$ and denote by
\begin{equation*}
  M_n = \int_{S^c} \phi\prt{x |m_n, \sigma_n} \dx
\end{equation*}
the mass probability of a gaussian random variable on $S^c$ with mean $m_n$ and  variance $\sigma_n^2$. The following inequality holds :
 \begin{equation}
M_n \lesssim \left( \sqrt { 2 } \sigma _ { n } / \sqrt { \pi } \gamma \right) \exp \left( - \gamma ^ { 2 } / 2 \sigma _ { n } ^ { 2 } \right)
 \end{equation}
 where $\lesssim$ denotes an inequality which is true asymptotically.
\end{boxtheorem}

\noindent
Since $$S^c \subseteq \intof{-\infty}{\mopt-\gamma} \cup \intfo{\mopt+\gamma}{+\infty}$$
we have by monotony of integral:
\begin{align*}
  M_n &\leq \int_{-\infty}^{\mopt-\gamma}\phi\prt{x |m_n, \sigma_n} \dx
  + \int_{\mopt+\gamma}^{+\infty}\phi\prt{x |m_n, \sigma_n} \dx\\
  &\leq \underbrace{\Phi \left( \frac { \mopt - m _ { n } } { \sigma _ { n } } - \frac { \gamma } { \sigma _ { n } } \right)+ 1-\Phi \left( \frac { \mopt - m _ { n } } { \sigma _ { n } } + \frac { \gamma } { \sigma _ { n } } \right)}_{:=A_n}
\end{align*}
where $\Phi\prt{\cdot}$ denotes the cumulative distribution function of a standard normal distribution. Since $\frac { p [ f ] - m _ { n } } { \sigma _ { n }}$ is bounded
 when $n$ goes to infinity and because $\Phi$ is continuous over $\R$ we have :
\begin{align*}
  A_n &\approx \Phi\prt{\frac{-\gamma}{\sigma_n}}+1-\Phi\prt{\frac{\gamma}{\sigma_n}}\\
  &= 2\cdot \Phi\prt{\frac{\gamma}{\sigma_n}}.
\end{align*}

Now we need to find an equivalent of $\Phi(x)$ when $x$ goes to infinity. Lets show the following inequality :
\begin{boxtheorem}
For $x>0$ :
\begin{equation}
  \label{eq:28}
  \frac{1}{x} \prt{1-\frac{1}{x^2}} \e^{-\frac{x^2}{2}}
  \leq \int_{x}^{+\infty} \e^{-\frac{t^2}{2}} \dt
  \leq   \frac{1}{x}  \e^{-\frac{x^2}{2}}
\end{equation}
\end{boxtheorem}

\noindent
By integration by parts, we have :
\begin{align}
  \int_{x}^{+\infty} \e^{-\frac{t^2}{2}} \dt &=
  \int_{x}^{+\infty} \frac{1}{t} \cdot \prt{t\cdot \e^{-\frac{t^2}{2}}} \dt \nonumber\\
  &= \left[ - \frac{\e^{-\frac{t^2}{2}}}{t} \right]_{x}^{+\infty}-
  \int_{x}^{+\infty} \frac{1}{t^2} \cdot \prt{\e^{-\frac{t^2}{2}}} \dt \nonumber\\
  &= \frac{1}{x}\cdot \e^{-\frac{x^2}{2}}- \int_{x}^{+\infty} \frac{1}{t^3} \cdot \prt{t\cdot \e^{-\frac{t^2}{2}}} \dt \label{eq:right}\\
  &= \frac{1}{x}\cdot \e^{-\frac{x^2}{2}} - \frac{1}{x^3}\cdot \e^{-\frac{x^2}{2}}
  + 3\int_{x}^{+\infty} \frac{1}{t^4} \cdot \e^{-\frac{t^2}{2}} \dt \nonumber\\
  &= \frac{1}{x} \prt{1-\frac{1}{x^2}} \cdot \e^{-\frac{x^2}{2}} + 3\int_{x}^{+\infty} \frac{1}{t^4} \cdot \e^{-\frac{t^2}{2}} \dt.\label{eq:left}
\end{align}
\noindent
\eqref{eq:right} proves the right inequality while \eqref{eq:left} proves the left one.
\noindent
\eqref{eq:28} is sufficient to shows that $\int_{x}^{+\infty} \e^{-\frac{t^2}{2}} \dt \sim \frac{1}{x}  \e^{-\frac{x^2}{2}}$, \ie
\begin{equation*}
A_n \sim 2 \frac{1}{\sqrt{2\pi}} \cdot \frac{\sigma_n}{\gamma} \e^{-\frac{\gamma^2}{\sigma_n^2}}= \sqrt{\frac{2}{\pi}}\cdot \frac{\sigma_n}{\gamma} \e^{-\frac{\gamma^2}{\sigma_n^2}}
\end{equation*}
\noindent
which concludes the proof of the intermediary proposition.

\noindent
Substituting $\mopt$ by $p[f]$, $m_n$ by the posterior mean $\hat { p } _ { \mathrm { FWBQ } } [ f ]$ and $\sigma_n$ by the posterior standard deviation $M M D \left( \left\{ x _ { i } , w _ { i } \right\} _ { i = 1 } ^ { n } \right)$ and using the results
 from Theorem 1 show the second theorem.

\section{Experimental Results}
\label{sec:ER}

\subsection{Overview of the code}

We have implemented the following algorithms : -- FW and FWBQ (when Line-Search mode is disabled) -- FWLS and FWLSBQ (when Line-Search mode is enabled). Two notebooks are available. The first notebook applies the different algorithms to a simulation study. The second notebook is an example of how the algorithms can be used to solve a problem in a specific area.

\subsection{Simulation Study}

We also used an exponentiated-quadratic (EQ) kernel $k(x, x') := \lambda^{2} exp(-\frac{1}{2}\sigma^{2} \| x-x' \|_{2}^{2} )$. EQ kernel is a relevant choice when $p$ is a mixture of gaussians. Moreover, the mean element $\mu_{p}$ has a closed-form expression.
In order to replicate the simulation study of the paper, we took $p$ as a mixture of 20 two-dimensional gaussians. Using two dimensions for each gaussians allow us to plot selected points, the function $f$, the approximation of the mean element $g_n$ and the mean element $\mu_{p}$ so that we can come up with a visual representation of the algorithms.
\newpage
The following graph is a plot of the bivariate kernel density estimate of the 20 two-dimensional gaussians. \\
\begin{center}
	\includegraphics[scale=0.25]{plot-simulation-kde.png}
\end{center}
We start with a smooth function $f$ that is the sum of 3 two-dimensional gaussians. Let us notice that the function $f$ intervenes at the end of our algorithm. In other words, we can change $f$ in order to obtain some other values of the integral without having to relaunch everything.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.25]{plot-simulation-3d-function.png}
	\caption{\textbf{KDE plot of the mixture of gaussians $p$.}}
\end{center}
\end{figure}
The next figures shows the 100 selected points in the FWBQ (FW algorithm) and the FWLSQB (FWLS algorithm) algorithms.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.20]{plot-simulation-selectedpoints-FWBQ.png}
	\includegraphics[scale=0.20]{plot-simulation-selectedpoints-FWLSBQ.png}
	\caption{\textbf{Points selected by the FW (left) and FWLS (right) algorithm}}
\end{center}
\end{figure}
It is interesting to visualize what is happening when the point are selected by the FW and the FWLS algorithms throughout the iterations. What is important is how fast the mean element is reconstructed by carefully choosing the points and their weights.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.30]{plot-simulation-3d-meanelement-FWBQ.png}
	\includegraphics[scale=0.30]{plot-simulation-3d-meanelement-FWLSBQ.png}
	\caption{\textbf{Convergence towards the mean element of FW algorithm (left) and the FWLS algorithm (right).}}
\end{center}
\end{figure}
Adding the line-search allows for a much faster convergence towards the mean element. In no more than 50 points, the approximation of the mean element seems to perfectly match the analytically calcultated mean element. We conclude by saying the line-search (which choose the $\rho_{i}$) plays a crucial role in the algorithm.
We can then compute the value of the integral with a $95\%$ confidence interval.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.25]{plot-simulation-mmdsquared-value-FWLSBQ.png}
	\caption{\textbf{MMD$^2$ and the value of the integral for the FWLSBQ algorithm.}}
\end{center}
\end{figure}

\subsection{An example of application}
In this section we have implemented from real data the two algorithms FWBQ and FWLSBQ. In order to have a basis of comparison, we have also implemented on our data the Monte Carlo integration. The data used represents the closing price of the shares of Apple, Facebook, Google and Microsoft of 2018. We compute the return of each stock. The return represents the design points of our theory. \\ \\
We want to calculate the payoff of a basket option : $$ \mathbb{E}_P \Big( f(S(T)) \Big) = \int f(s) p(s) ds $$ where for example $f(x) = \max \acc{ \sum_{i = 1}^{d} \alpha_{(i)} x_{(i)} - K, 0} $ with $K = \sum_{i = 1}^{d} \alpha_{(i)} S_{(i)}(0) $. The idea is to approximate this expectancy by the one given by FWLSBQ : $$ \mathbb{E}_P \Big( f(S(T)) \Big) \simeq \sum_{i = 1}^{N} w_{(i)}^{BQ} f(S_{(i)}^{FW}(T))) $$ where the $w_{(i)}^{BQ}$ are the BQ weights and the $S_{(i)}^{FW}(T)$ are the points selected by the FWLS algorithm. \\
We need a model for the stock prices. The evolution is modelled by the following stochastic differential equation : $$ dS(t) = \mu dS_t + \sigma S_t dB(t) $$ where $S$ is the asset price, $\mu$ is the drift, $\sigma$ is the volatility and $B$ is a Brownian motion\footnote{which can be thought as $dB(t) \sim \gauss{0}{\dt}$.}. Let $r$ be the risk-free rate. After using Ito's Lemma to get an expression of $d\text{log}S_t$, the solution is given by : $$S(t) = S(0) \cdot \exp \acc{\prt{r-\frac{1}{2}\sigma^2}\cdot t + \sigma \sqrt{t} \cdot \gauss{0}{1} }.$$ In the case where we are studying a basket of $d$ assets, $S_{(i)}(t) = S_{(i)}(0) \cdot \exp \acc{(r-\frac{1}{2}\sigma_{(i)}^2)t + \sigma \sqrt{t} W_{(i)} }$ for $i \in \{0, ..., d-1\}$ where the vector $W$ = $(W_{(0)}, ..., W_{(d-1)})$ follows a multivariate normal distribution with mean $MU = (0, ..., 0)$ and with a covariance matrix $COR$ which is in fact the correlation matrix between assets. \\
The $d$ assets will be $d$ stocks. We first start by estimating the covariance matrix between the $d = 4$ assets (Apple, Facebook, Google and Microsoft) using the close on a 1-year history (year 2018). \\
\\
We took some arbitrary values for $r$ ($5\%$) and $T$ ($10$). We launched the FWLSBQ algorithm. We obtained the following results.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.25]{plot-application-mmdsquared-value-FWLSBQ.png}
	\caption{\textbf{MMD$^2$ and the value of the integral for the FWLSBQ algorithm.}}
\end{center}
\end{figure}
This graph is an exemple of how a real dataset can be used to model a distribution which will be then used to price any payoff given some known parameters such as $r$ and $T$.
\\ \\
In finance, both giving a fast result and quantifying a result are useful. We used normal distributions to model the returns while we know that financial returns are leptokurtic. Modelling these returns with student distributions for example may be more realistic. However, we will need to use a kernel such that the mean element is analytically tractable.

\subsection{Discussion about our implementation}
Our implementation is a bit slow. A way to speed the implementation would be to use parallel computing for finding the point \eqref{eq:xmin} at each iteration. For exemple, if we are calculating the value of $M$ points of the space, we can compute $X$ threads each responsible for $M/X$ estimations in order to accelerate the process. Moreover, we believe that the way we keep track of the successive approximations of the mean element $g_{i}$ can also be improved. We used a dictionnary of functions created by a function returning the convex-combination (with weights $1-\rho_{i}$ and $\rho_{i}$) of the previous function $g_{i-1}$ and the atom $\overline{g_i}$. So, each time, we call a function, successive calculations are made to return a value. The higher the iteration, the longer it takes to return a value.

% intuition
% graph to show that it is closed to a gradient descent
% drawing line search Aspremont

\section*{Resources}
\label{sec:bibli}

\nocite{*}
\printbibliography[heading=none]
