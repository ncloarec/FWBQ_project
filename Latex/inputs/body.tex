\section*{Notations}
We start with some notations we will use along this report.
\begin{itemize}[font= \color{blue} \large, label= $\bullet$]
 \item For any function, $g(\cdot)$ denotes the function $g: x \mapsto g(x)$.
 \item In integrals, $\d x$ denote $\d \lambda(x)$, \ie with respect to the Lebesgue measure.
\end{itemize}

\section*{Introduction}

The goal of the article \cite{FWBQ} is to compute efficiently the integrals of the form
$ \displaystyle \int _ { \X } f ( x ) p ( x ) \mathrm { d } x$
where $\X \subseteq \R ^ { d }$ is a measurable space,
$d \geq 1$ integer representing the dimension of the problem, $p$ a probability
density with respect to the Lebesgue measure on $\X$ and $f : \X \rightarrow \R$
 is a \textit{test}-function.

 We will use the common approximation
 \begin{boxproblem}
   \begin{equation}
  \int _ { \X } f ( x ) p ( x ) \d x \approx \sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right)
   \end{equation}
 \end{boxproblem}

 but of course the real challenge lies in the choice of sequences $\acc{x_i}$ and
 $\acc{w_i}$ :
  \begin{mydescription}
    \item [Monte Carlo]: $w_i = \frac{1}{n}$ and $x_i$ realization of multivariate
    random variable $X_i \stackrel{iid}{\sim} X$ where $X$ has $p(\cdot)$ as probability
    distribution.
    \item [Kernel herding]:
    \item [Quasi-Monte Carlo]:


  \item [Frank-Wolfe Bayesian Quadrature]:
  \begin{myitemize}[0.2cm]
\item $\acc{w_i}$ appear naturally in the Bayesian Quadrature by taking the expectation of a posterior distribution  (described in section \ref{sec:BQ}),
\item $\acc{x_i}$ are selected by the Frank-Wolfe algorithm in order to minimize a posterior variance (described in section \ref{sec:FW}).
\end{myitemize}
  \end{mydescription}
  The main interest of the method developed in \cite{FWBQ} is the super fast
  \textit{exponential} convergence to the true value of the integral compared to the other methods mentioned above.\\

\noindent
  Through this report, we will detail every results from \cite{FWBQ} with the goal
  to clarify and explain details that could have been omitted intentionally or not and which, in our view, make the Briol's and al. approach more natural, intuitive and easier
  to understand.
  % use Bach presentation for convergence rates benchmark

\section{Background}
% write $\mu_p$ with an integral
%WHY RKHS, WHY so USEFUL ?
Let $\X \subseteq \R ^ { d }$ be a measurable space, $\mu$ a measure on $\X$ such
that $p = \frac{\d \mu}{ \d \lambda}$ where $\lambda$ denotes the Lebesgue measure on $\X$,
  $\H \subset L^2(\X, \R; \mu)$ be an RKHS with a
 reproducing kernel $k: \X \times \X \rightarrow \R$, $\Phi$ its canonical feature
 map associated. We denote respectively by $\bigps{\cdot}{\cdot}$ and $\norme{\cdot}$
 the bigps product and norm induced on $\H$.

 Recall that the following relations hold:
 \begin{boxexercise}
   \begin{align}
  \forall x \in \X , \quad &k ( \cdot , x ) \in \H\\
  \forall x \in \X , \forall f \in \H , \quad &\langle f , k ( \cdot , x ) \rangle _ { \H } = f ( x )\\
  \forall (x,y) \in \X^2 \quad &k(x,y) = \bigps{\Phi(x)}{\Phi(y)}
  \end{align}
\end{boxexercise}


Let's denote as \cite{FWBQ}:
\begin{boxdefinition}
  \begin{align*}
    p[f] &:= \int _ { \X } f ( x ) \mathrm { d } \mu(x) = \int _ { \X } f ( x ) p ( x ) \mathrm { d } x\\
    \p[f]&:=\sum _ { i = 1 } ^ { n } w _ { i } f \left( x _ { i } \right).
  \end{align*}
\end{boxdefinition}


We will use the \textit{maximum mean discrepancy} (MMD) as our main metric to measure
 the accuracy of the approximation $p[f] \approx \p[f]$ in the worst case scenario and which is defined as
 \begin{boxdefinition}
    $$\MMD : = \sup _ { f \in \H : \norme{f}=1 } | p [ f ] - \hat { p } [ f ] |.$$
 \end{boxdefinition}

 Let's show (formula 3. in \cite{FWBQ}) that MMD can be rewrite as
 \begin{boxtheorem}
   \begin{align}
     \MMD = \left\| \mu _ { p } - \mu _ { \hat { p } } \right\| _ { \mathcal { H } }
   \end{align}
   where $\mu_p(\cdot) = p[\Phi(\cdot)]$ and $\mu_{\p}(\cdot) = \p[\Phi(\cdot)]$.
 \end{boxtheorem}

   \begin{itemize}[leftmargin=*, font= \color{blue} \large, label= $\bullet$]
     \item \textbf{For all $f$ in $\H$, we have $p[f] = \bigps{f}{\mu_p}$.} By using
     the dirac delta function, the continuity of the inner product and viewing integral
     as a limit of a sum, we get
     \begin{boxcomputation}
     \begin{align*}
       p[f] &= \int_{\X} f(x) \d \mu(x)\\
       &= \int_{\X} \delta_x[f]  \d \mu(x)\\
       &= \int_{\X} \bigps{f}{\Phi(x)}  \d \mu(x)\\%TODO Not obvious
       &=  \bigps{f}{\int_{\X} \Phi(x) \d \mu(x)} \\
       &= \bigps{f}{\mu_p}
     \end{align*}
      \end{boxcomputation}
     \item \textbf{For all $f$ in $\H$, we have $\p[f] = \bigps{f}{\mu_{\p}}$.}
     \begin{boxcomputation}
     \begin{align*}
       \p[f] &= \sum_{i=1}^n w_i f(x_i)\\
       &= \sum_{i=1}^n w_i \delta_{x_i}[f]\\
       &= \sum_{i=1}^n w_i \bigps{f}{\Phi(x_i)}\\%TODO Not obvious
       &=  \bigps{f}{\sum_{i=1}^n w_i \Phi(x_i)} \\
       &= \bigps{f}{\mu_{\p}}
     \end{align*}
   \end{boxcomputation}
\item By using previous results and the Cauchy-schwartz inequality, we get :
\begin{boxcomputation}
\begin{align*}
\MMD &= \sup _ { f \in \H : \norme{f} = 1 } \abs{
\bigps{f}{\mu_{p}-\mu_{\p}} }\\
&\leq \sup _ { f \in \H : \norme{f} = 1 } \norme{f} \norme{\mu_{p}-\mu_{\p}}\\
&= \norme{\mu_{p}-\mu_{\p}}
\end{align*}
 \end{boxcomputation}
with equality if and only if $f$ and  $\mu_{p}-\mu_{\p}$ are linearly dependent.
We deduce the desired result by taking $ \displaystyle f = \frac{1}{\norme{\mu_{p}-\mu_{\p}}}\prt{\mu_{p}-\mu_{\p}}$.

   \end{itemize}



% on maximise avec cauchy schwartz + égalité

\section{Bayesian Quadrature}
\label{sec:BQ}
Let's place a functional prior on the integrand $f$ and denote by $(\Omega, \mathcal{F}, \P)$ its probability space associated. We will assume that $f$ to be
 a \textit{centered} \textbf{gaussian process} with the kernel $k$ as its covariance
 function, \ie
 \begin{boxexample}
   \begin{align*}
     \forall x \in \H, \quad  &\Expec{f(x)} = 0\\
     \forall x, y \in \H, \quad  &\Cov{f(x)}{f(y)} = k(x,y)
   \end{align*}
 \end{boxexample}
A useful property is that $p[f]$ is a gaussian variable and then completely defined by its second-order statistics:
\begin{boxtheorem}
  \begin{align}
    \Expec{p[f]} &= 0\\
    \Var{p[f]} &= \int_{\X^2} k(x,y) \d \mu(x) \d \mu(y)
  \end{align}
\end{boxtheorem}


  By switching integrals using Fubini's theorem, we get
  \begin{boxcomputation}
    \begin{align*}
      \Expec{p[f]} &= \int_{\Omega} p[f](w) \d \P(w)\\
      &= \int_{\Omega} \int_{\X} f(x,w) \d \mu(x)  \d \P(w)\\
      &= \int_{\X} \underbrace{\int_{\Omega}  f(x,w) \d \P(w)}_{\Expec{f(x)}=0} \d \mu(x)  =0
    \end{align*}
\end{boxcomputation}
\begin{boxcomputation}
  \begin{align*}
      \Var{p[f]} &= \Expec{p[f]^2} = \int_{\Omega} p[f](w)^2 \d \P(w)\\
      &=\int_{\Omega} \prt{\int_{\X} f(x,w) \d \mu(x)}^2 \d \P(w)\\
      &=\int_{\Omega} \int_{\X^2} f(x,w)f(y,w) \d \mu(x) \d \mu(y) \d \P(w)\\
      &= \int_{\X^2} \underbrace{\int_{\Omega}  f(x,w)f(y,w) \d \P(w)}_{=\Cov{f(x)}{f(y)} = k(x,y)} \d \mu(x) \d \mu(y) \\
      &= \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y)
    \end{align*}
\end{boxcomputation}

Assume that samples $\acc{x_i}$ and $\acc{f_i} := \acc{f(x_i)}$ are given for $i=1$ to $n$ and denote by $K:= \prt{k(x_i, x_j)}_{1 \leq i,j \leq n}$. A natural question arises: how to update the weights $\acc{w_i}_{i=1}^n $?

\begin{figure}[H]
\centering
\includegraphics[scale=.33]{bq.png}
\caption{\textbf{An illustration of Bayesian Quadrature.}\\\textit{Source:} \cite{huszar}}
\label{fig:bq}
\end{figure}

First of all, let's determine the conditional distribution $\displaystyle
 p[f]| \; \mathbf{f}$ where
$\mathbf{f} = \transpose{\prt{f_1, \ldots, f_n}}$.
Since both $p[f]$ and $\mathbf{f}$ are gaussian, we can use the conditional gaussian rule:
\begin{boxdefinition}
  By denoting, $y_1 := p[f]$, $y_2 = \mathbf{f}$, $y = \begin{pmatrix}
    y_1\\y_2
\end{pmatrix}$, $\displaystyle \Sigma = \begin{pmatrix}
  \Sigma_{1,1}, \Sigma_{1,2}\\
  \Sigma_{2,1}, \Sigma_{2,2}
\end{pmatrix}$ its covariance matrix by blocks, we have :
$$p[f]| \; \mathbf{f} \sim \mathcal { N } (  \mu  , \Stilde )$$
where

  %
  %

  \begin{equation*}
  \begin{cases}
    \mu \;&= \;  \Sigma _ { 12 } \; \Sigma _ { 22 } ^ { - 1 } \; \mathbf{f} \\
     \Stilde \; &= \;  \Sigma _ { 11 } \;- \Sigma _ { 12 } \;\Sigma _ { 22 } ^ { - 1 } \; \Sigma _ { 21 }.
  \end{cases}
\end{equation*}
\end{boxdefinition}

Let's determine what $\mu$ and $\Stilde$ look like in our context.
  \begin{boxcomputation}
\begin{align*}
  \Sigma _ { 22 } &= \prt{\Cov{f_i}{f_j}}_{1 \leq i,j \leq n}\\
  &= \prt{\Cov{f(x_i)}{f(x_j)}}_{1 \leq i,j \leq n}\\
  &= \prt{k(x_i, x_j)}_{1 \leq i,j \leq n}\\
  &= K\\
  \mu &= \begin{pmatrix}
    \Cov{p[f]}{f_1} \\ \ldots \\ \Cov{p[f]}{f_n}\\
\end{pmatrix} K^{-1} \mathbf{f}
\end{align*}
\end{boxcomputation}
Let's rewrite the vector from the left:
\begin{boxcomputation}
\begin{align*}
\Cov{p[f]}{f_i} &= \int_{\Omega} p[f](w) f(x_i, w) \d \P(w)\\
&= \int_{\Omega} \int_{\X} f(x, w) \d \mu(x) f(x_i, w) \d \P(w)\\
&=  \int_{\X}  \underbrace{\int_{\Omega} f(x, w) f(x_i, w) \d \P(w)}_{=\Cov{f(x)}{f(x_i)}=k(x,x_i)} \d \mu(x) \\
&= \int_{\X} k(x,x_i)\d \mu(x)\\
&= \int_{\X} \Phi(x_i)(x) \d \mu(x)\\
&= p[\Phi(x_i)]\\
&= \mu_p \prt{x_i}\\
\end{align*}
\end{boxcomputation}

By denoting $z := \prt{z_i}_{i=1}^n = \prt{\mu_p \prt{x_i}}_{i=1}^n \in \R^n$, we
 get the desired formula\footnote{formula 4 in \cite{FWBQ} } for the expectation:
 \begin{boxtheorem}
   \begin{equation}
     \mu = z ^ { T } K ^ { - 1 } \mathbf { f }
   \end{equation}
 \end{boxtheorem}

The variance is then straightforward :
\begin{boxexample}
  \begin{align*}
    \Stilde &= \Var{p[f]} - z ^ { T } K ^ { - 1 } z\\
    &= \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y) - z ^ { T } K ^ { - 1 } z
  \end{align*}
\end{boxexample}

\begin{boxcomputation}
  We need to simplify the integral to obtain the desired formula for the variance:
\begin{align*}
  \int_{\X^2}  k(x,y) \d \mu(x) \d \mu(y) &= \int_{\X} \int_{\X} \Phi(x)(y) \d \mu(y) \d \mu(x) \\
  &= \int_{\X} p[\Phi(x)] \d \mu(x) \\
  &= \int_{\X} \mu_p\prt{x} \d \mu(x) \\
  &=  p[\mu_p]
\end{align*}
\end{boxcomputation}
which leads us to the desired result:
\begin{boxtheorem}
  $$\Stilde = p[\mu_p] -z ^ { T } K ^ { - 1 } z$$
\end{boxtheorem}

Both the conditional expectation and variance are essentials in the FWBQ\footnote{Frank-Wolfe Bayesian Quadrature} algorithm:
\begin{itemize}[font= \color{blue} \large, label= $\bullet$]
  \item $\mu = z ^ { T } K ^ { - 1 } \mathbf { f }$ which can be written as
  $\displaystyle \mu = \sum_{i=1}^n w_i^{BQ} f_i$ where $w^{BQ} := \transpose{\prt{K^{-1}}}z$. $\mu$ appears to be the \textbf{most natural estimation of our integral} $p[f]$.
  It also gives us the \textbf{new weights to update}.
  \item $\Stilde = p[\mu_p] -z ^ { T } K ^ { - 1 } z$ which is also equal to $\MMD^2$
  according to \cite{huszar} and can be interpreted as our \textbf{uncertainty}. Therefore
  we need to \textbf{choose $\acc{x_i}$ which minimize this quantity}. This will be achieved
  by using the Frank Wolfe algorithm described in the next section.
\end{itemize}

% Conditional rule demonstrated
% Demonstration formula 4
% Demonstration formula 5

\section{Frank-Wolfe algorithm}
\label{sec:FW}

Let's $J$ be a convex differentiable real-valued function on a
domain $\G\subset\H$ which is supposed to be a compact and convex.

The Frank-Wolfe algorithm \cite{fwalgo} proposes a method to solve the following optimization
 problem :

\begin{boxproblem}
  \begin{equation} \label{eq:fwpbm}
    \begin{array} { l } { \text { Minimize } \acc{J ( \mathbf { x } )} } \\ { \text { subject to } \mathbf { x } \in \G } \end{array}
  \end{equation}
\end{boxproblem}
Let's describe here both the Frank-Wolfe algorithm and its variant with the Line Search :
\begin{boxprogramming}
\begin{mydescription}
  \item [Initialization] $g _ { 1 } = \overline { g } _ { 1 } \in \mathcal { G }$ and
  step-size sequence $\acc{\rho_i}_{i=1}^n$ (not required in the FWLS algorithm).
  \item [Iterations] For $i=2$ to $n$ :
  \begin{mydescription}[0.2cm]
    \item [Search direction] we replace in \eqref{eq:fwpbm} $J$ by its first-order
    Taylor expansion around $g_k$ to solve the following subproblem
    \begin{boxcomputation}
      \begin{equation}\label{eq:fwlin}
        \begin{array} { c } { \text { minimize }  \acc{J \left( g_{i-1} \right) + \nabla J \left( g_{i-1} \right) ^ { \mathrm { T } } \left( g - g_{i-1} \right)} } \\ { \text { subject to } g \in \G } \end{array}
      \end{equation}
    \end{boxcomputation}
Let's denote as \cite{FWBQ}
\begin{equation}\label{eq:gbar}
  \overline { g } _ { i } := \arg \min _ { g \in \G } \bigps{g}{ \nabla J \left( g _ { i - 1 } \right)}
\end{equation}
 where we have removed
 in \eqref{eq:fwlin} terms independent of the optimization variable.
 \item [New iteration point]: we choose $g_i$ as a \textit{convex}-combination of
 $g_{i-1}$ and $\overline{g_i}$.
 \begin{equation}
   g _ { i } = \left( 1 - \rho _ { i } \right) g _ { i - 1 } + \rho _ { i } \overline{ g } _ { i }
 \end{equation}
 \begin{mydescription}
\item [FW] $\rho _ { i }$ is  determined by the sequence given at the initialization
\item [FWLS]
\begin{equation*}
\rho _ { i } := \operatorname { argmin } _ { \rho \in [ 0,1 ] } J \left( ( 1 - \rho ) g _ { i - 1 } + \rho \overline{g _ { i }} \right)
\end{equation*}

 \end{mydescription}
  \end{mydescription}
\end{mydescription}
\end{boxprogramming}

\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-9,-4) rectangle (0.24,2);
\fill[line width=2pt,color=ggyellow,fill=ggyellow,fill opacity=0.10000000149011612] (-5.58,1.24) -- (-3.52,1.34) -- (-1.08,-1.24) -- (-4.38,-3.46) -- (-8.1,-2.2) -- cycle;
\draw [color=ggyellow] (-5.58,1.24)-- (-3.52,1.34);
\draw [color=ggyellow] (-3.52,1.34)-- (-1.08,-1.24);
\draw [color=ggyellow] (-1.08,-1.24)-- (-4.38,-3.46);
\draw [color=ggyellow] (-4.38,-3.46)-- (-8.1,-2.2);
\draw [color=ggyellow] (-8.1,-2.2)-- (-5.58,1.24);
\draw [->] (-4.42,-0.94) -- (-3.02,-0.1);
\draw [dotted] (-4.42,-0.94)-- (-7.29,-2.47);
\draw [dotted] (-4.42,-0.94)-- (-8.1,-2.2);
\draw [->] (-5.73,-1.38) -- (-4.42,-1.84);
\draw [dotted] (-5.41,-1.47)-- (-7.10,-0.84);
  \node[draw] at (-3.02,0.3)  (grad1) {$\mathbf{\nabla J(g_1)}$};
  \node[draw] at (-4.22,-2.3)  (grad2) {$\mathbf{\nabla J(g_2)}$};
\draw [fill=blue] (-4.42,-0.94) circle (2pt) node[below] {$\mathbf{g_1}$};
\draw [fill=blue] (-8.1,-2.2) circle (2pt) node[shift={(-1.5ex,-2ex)}] {$\mathbf{\overline{g_2}=\overline{g_3}}$};
\draw [fill=blue] (-5.73,-1.38) circle (2pt) node[shift={(0ex,-2.5ex)}] {$\mathbf{g_2}$};
\draw [fill=blue] (-6.32,-1.59) circle (2pt) node[shift={(-0.5ex,2ex)}] {$\mathbf{g_3}$};
\draw[->,ggred, looseness=2] (-4.42,-0.94) .. controls (0,-2.3) and  (-4,-4.5) .. node[sloped] {} (-8.1,-2.2);
\draw[->,ggred] (-8.1,-2.2) -- (-5.73,-1.38);
\draw[->,ggred, looseness=2] (-5.73,-1.38) .. controls (-5.6,0.5) .. node[sloped] {} (-8.1,-2.2);
\draw[->,ggred] (-8.1,-2.2) -- (-6.32,-1.59);
\end{tikzpicture}
\begin{center}
  \textbf{An illustration of the Frank Wolfe algorithm. }
\end{center}
  In equation \eqref{eq:gbar}, we don't have necessarily $\overline{g_i} \in \spn{\nabla g_{i-1}}$. In fact, we can clearly see in the illustration above
   that the point
   \begin{align*}
  \widetilde { g } _ { i } &:= \arg \min _ { g \in \G\cap  \spn{\nabla g_{i-1}}} \bigps{g}{ \nabla J \left( g _ { i - 1 } \right)} \\
  &= \arg \min _ { \alpha \geq 0}
  \acc{-\alpha \nabla g_{i-1} \mid \alpha \nabla g_{i-1} \in \G}
\end{align*}
would not be optimal as $\inner{\overline { g } _ { i }}{\nabla g_{i-1}}
< \inner{\widetilde { g } _ { i }}{\nabla g_{i-1}}$.


% \hspace{-1.5cm}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-9,-4) rectangle (0.24,2);
\fill[line width=2pt,color=ggyellow,fill=ggyellow,fill opacity=0.10000000149011612] (-5.58,1.24) -- (-3.52,1.34) -- (-1.08,-1.24) -- (-4.38,-3.46) -- (-8.1,-2.2) -- cycle;
\draw [color=ggyellow] (-5.58,1.24)-- (-3.52,1.34);
\draw [color=ggyellow] (-3.52,1.34)-- (-1.08,-1.24);
\draw [color=ggyellow] (-1.08,-1.24)-- (-4.38,-3.46);
\draw [color=ggyellow] (-4.38,-3.46)-- (-8.1,-2.2);
\draw [color=ggyellow] (-8.1,-2.2)-- (-5.58,1.24);
\draw [->] (-4.42,-0.94) -- (-3.02,-0.1);
\draw [dotted] (-4.42,-0.94)-- (-7.29,-2.47);
\draw [->] (-5.41,-1.47) -- (-4.42,-1.84);
\draw [dotted] (-5.41,-1.47)-- (-7.10,-0.84);
  \node[draw] at (-3.02,0.3)  (grad1) {$\mathbf{\nabla J(g_1)}$};
  \node[draw] at (-4.22,-2.3)  (grad2) {$\mathbf{\nabla J(g_2)}$};
\draw [fill=blue] (-4.42,-0.94) circle (2pt) node[below] {$\mathbf{g_1}$};
\draw [fill=blue] (-7.294,-2.47) circle (2pt) node[shift={(-1.5ex,-1.5ex)}] {$\mathbf{\widetilde{g_2}}$};
\draw [fill=blue] (-5.41,-1.47) circle (2pt) node[shift={(0ex,-2.5ex)}] {$\mathbf{g_2}$};
\draw [fill=blue] (-7.10,-0.846) circle (2pt) node[shift={(-1.5ex,1.5ex)}] {$\mathbf{\widetilde{g_3}}$};
\draw [fill=blue] (-6.01,-1.246) circle (2pt) node[shift={(0ex,2.5ex)}] {$\mathbf{g_3}$};
\draw[->,ggred, looseness=2] (-4.42,-0.94) .. controls (0,-2.3) and  (-4,-4.5) .. node[sloped] {} (-7.29,-2.47);
\draw[->,ggred] (-7.29,-2.47) -- (-5.41,-1.47);
\draw[->,ggred, looseness=2] (-5.41,-1.47) .. controls (-5.6,0.5) .. node[sloped] {} (-7.10,-0.846);
\draw[->,ggred] (-7.10,-0.846) -- (-6.01,-1.246);
\end{tikzpicture}
\begin{center}
  \textbf{Comparison with a descent algorithm. }
\end{center}



% intuition
% graph to show that it is closed to a gradient descent
% drawing line search Aspremont

\section*{Resources}
\label{sec:bibli}

\nocite{*}
\printbibliography[heading=none]
